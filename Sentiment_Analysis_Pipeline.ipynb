{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIraq War Newspaper Sentiment Analysis\\nversion: M. W-C. Wong, 2019-05-09\\n\\nPython ver. 3.6.4\\nnumpy ver. 1.16.0\\npandas ver. 0.23.4\\nseaborn ver. 0.9.0\\nmatplotlib ver. 2.1.2\\nscikit-learn ver. 0.20.3\\nXGBoost ver. 0.82\\nlightgbm ver. 2.2.3\\nkeras ver. 2.2.4\\n\\n\\nOverview:\\n            Scikit-learn based pipelines for Iraq War newspaper article sentiment analysis. Coded for binary optimism\\n            vs. pessimism classification.\\n            \\n            \\n\\n\\nNotes:     (1) Python 3.6 Anaconda\\n           (2) Encoding UTF-8\\n           (3) Runs from directory with the following files\\n                            lipad-house-senate-doc2vec-42nd.csv\\n           (4) Each classifier is trained on two interlocking pipelines, one for the preprocessing column transformer \\n               and the other for the classifier/preprocessor training process\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Iraq War Newspaper Sentiment Analysis\n",
    "version: M. W-C. Wong, 2019-05-09\n",
    "\n",
    "Python ver. 3.6.4\n",
    "numpy ver. 1.16.0\n",
    "pandas ver. 0.23.4\n",
    "seaborn ver. 0.9.0\n",
    "matplotlib ver. 2.1.2\n",
    "scikit-learn ver. 0.20.3\n",
    "XGBoost ver. 0.82\n",
    "lightgbm ver. 2.2.3\n",
    "keras ver. 2.2.4\n",
    "\n",
    "\n",
    "Overview:\n",
    "            Scikit-learn based pipelines for Iraq War newspaper article sentiment analysis. Coded for binary optimism\n",
    "            vs. pessimism classification.\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "Notes:     (1) Python 3.6 Anaconda\n",
    "           (2) Encoding UTF-8\n",
    "           (3) Runs from directory with the following files\n",
    "                            lipad-house-senate-doc2vec-42nd.csv\n",
    "           (4) Each classifier is trained on two interlocking pipelines, one for the preprocessing column transformer \n",
    "               and the other for the classifier/preprocessor training process\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Michael/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ver. 3.6.4\n",
      "numpy ver. 1.16.0\n",
      "pandas ver. 0.23.4\n",
      "seaborn ver. 0.9.0\n",
      "matplotlib ver. 2.1.2\n",
      "scikit-learn ver. 0.20.3\n",
      "XGBoost ver. 0.82\n",
      "lightgbm ver. 2.2.3\n",
      "keras ver. 2.2.4\n"
     ]
    }
   ],
   "source": [
    "############### PYTHON 3\n",
    "\n",
    "\n",
    "\n",
    "#Module Imports\n",
    "import platform\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import re\n",
    "import json\n",
    "import lightgbm\n",
    "import keras\n",
    "import xgboost\n",
    "import pickle\n",
    "import mglearn\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from joblib import dump, load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dask_searchcv import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "print(\"Python ver. \"+platform.python_version())\n",
    "print(\"numpy ver. \"+np.__version__)\n",
    "print(\"pandas ver. \"+pd.__version__)\n",
    "print(\"seaborn ver. \"+sns.__version__)\n",
    "print(\"matplotlib ver. \"+matplotlib.__version__)\n",
    "print(\"scikit-learn ver. \"+sklearn.__version__)\n",
    "print(\"XGBoost ver. \"+xgboost.__version__)\n",
    "print(\"lightgbm ver. \"+lightgbm.__version__)\n",
    "print(\"keras ver. \"+keras.__version__)\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment Score</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>Just woke up. Having no school is the best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment Score  \\\n",
       "1599995  4                 \n",
       "1599996  4                 \n",
       "1599997  4                 \n",
       "1599998  4                 \n",
       "1599999  4                 \n",
       "\n",
       "                                                                                  Tweet  \n",
       "1599995  Just woke up. Having no school is the best feeling ever                         \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta  \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me for details                        \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur                \n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H                   "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Read-in, ignore unnecessary columns\n",
    "data = pd.read_csv(\"Data/twitter_sentiment_data/training.1600000.processed.noemoticon.csv\",encoding = \"ISO-8859-1\",usecols=[0,5],names=[\"Sentiment Score\",\"Tweet\"])\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Sentiment Score\n",
      "count  1.600000e+06   \n",
      "mean   2.000000e+00   \n",
      "std    2.000001e+00   \n",
      "min    0.000000e+00   \n",
      "25%    0.000000e+00   \n",
      "50%    2.000000e+00   \n",
      "75%    4.000000e+00   \n",
      "max    4.000000e+00   \n",
      "1600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Michael/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1706: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2ad8169f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Initial Data Visualization\n",
    "print(data.describe())\n",
    "print(len(data))\n",
    "#print(data[\"Sentiment Score\"].tolist())\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[14,7])\n",
    "sentiment_histogram = sns.distplot(data[\"Sentiment Score\"].tolist(), kde=False)#, height=7, aspect=0.9)\n",
    "sns.set()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.title(\"Distribution of Sentiment Scores\")\n",
    "plt.grid()\n",
    "#plt.savefig('figures\\sns fig1.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@octolinz16 It it counts, idk why I did either. you never talk to me anymore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>about to file taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@LettyA ahh ive always wanted to see rent  love the soundtrack!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@alydesigns i was out most of the day so didn't get much done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>@angry_barista I baked you a cake but I ated it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>this week is not going as i had hoped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>blagh class at 8 tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I hate when I have to call and wake people up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Just going to cry myself to sleep after watching Marley and Me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>im sad now  Miss.Lilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ooooh.... LOL  that leslie.... and ok I won't do it again so leslie won't  get mad again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Meh... Almost Lover is the exception... this track gets me depressed every time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599970</th>\n",
       "      <td>Thanks @eastwestchic &amp;amp; @wangyip Thanks! That was just what I was looking for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599971</th>\n",
       "      <td>@marttn thanks Martin. not the most imaginative interface, but it'll do for now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599972</th>\n",
       "      <td>@MikeJonesPhoto Congrats Mike  Way to go!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599973</th>\n",
       "      <td>http://twitpic.com/7jp4n - OMG! Office Space... I wanna steal it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599974</th>\n",
       "      <td>@yrclndstnlvr ahaha nooo you were just away from everyone else! i had to see Kara, she'd die. and yess we aree, ill see you saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599975</th>\n",
       "      <td>@BizCoachDeb  Hey, I'm baack! And, thanks so much for all those kind notes while I was gone. They made me smile at times when I needed it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599976</th>\n",
       "      <td>@mattycus Yeah, my conscience would be clear in that case.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599977</th>\n",
       "      <td>@MayorDorisWolfe Thats my girl - dishing out the &amp;quot;advice&amp;quot;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599978</th>\n",
       "      <td>@shebbs123 i second that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599979</th>\n",
       "      <td>In the garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599980</th>\n",
       "      <td>@myheartandmind jo jen by nemuselo zrovna tÃ© holce ael co nic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599981</th>\n",
       "      <td>Another Commenting Contest! [;: Yay!!!  http://tinyurl.com/m6j2an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599982</th>\n",
       "      <td>@thrillmesoon i figured out how to see my tweets and facebook status updates, and i was set  the groups seemed like a pain to set up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599983</th>\n",
       "      <td>@oxhot theri tomorrow, drinking coffee, talking about our most important and favourite issue! YOU know what I mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599984</th>\n",
       "      <td>You heard it here first -- We're having a girl. Hope it has my looks and Wendy's brains. (Kidding, babe).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599985</th>\n",
       "      <td>if ur the lead singer in a band, beware falling prey to LSD &amp;quot;Lead Singer Disease&amp;quot; http://tinyurl.com/n65xjt  #music #haveyouever?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599986</th>\n",
       "      <td>@tarayqueen too much ads on my blog.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599987</th>\n",
       "      <td>@La_r_a NEVEER  I think that you both will get on well with each other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599988</th>\n",
       "      <td>@Roy_Everitt ha- good job. that's right - we gotta throw that #bigrun tag EVERYWHERE! I wanna get it trending before I start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599989</th>\n",
       "      <td>@Ms_Hip_Hop im glad ur doing well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599990</th>\n",
       "      <td>WOOOOO! Xbox is back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599991</th>\n",
       "      <td>@rmedina @LaTati Mmmm  That sounds absolutely perfect... but my schedule is full. I won't have time to lay in bed until Sunday. Ugh!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599992</th>\n",
       "      <td>ReCoVeRiNg FrOm ThE lOnG wEeKeNd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599993</th>\n",
       "      <td>@SCOOBY_GRITBOYS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>@Cliff_Forster Yeah, that does work better than just waiting for it  In the end I just wonder if I have time to keep up a good blog.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>Just woke up. Having no school is the best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                               Tweet\n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D                        \n",
       "1        is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!                            \n",
       "2        @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                                                  \n",
       "3        my whole body feels itchy and like its on fire                                                                                             \n",
       "4        @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.                             \n",
       "5        @Kwesidei not the whole crew                                                                                                               \n",
       "6        Need a hug                                                                                                                                 \n",
       "7        @LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                                        \n",
       "8        @Tatiana_K nope they didn't have it                                                                                                        \n",
       "9        @twittera que me muera ?                                                                                                                   \n",
       "10       spring break in plain city... it's snowing                                                                                                 \n",
       "11       I just re-pierced my ears                                                                                                                  \n",
       "12       @caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                                             \n",
       "13       @octolinz16 It it counts, idk why I did either. you never talk to me anymore                                                               \n",
       "14       @smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.                      \n",
       "15       @iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!                                    \n",
       "16       Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                                              \n",
       "17       about to file taxes                                                                                                                        \n",
       "18       @LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                                           \n",
       "19       @FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                                             \n",
       "20       @alydesigns i was out most of the day so didn't get much done                                                                              \n",
       "21       one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh*                                       \n",
       "22       @angry_barista I baked you a cake but I ated it                                                                                            \n",
       "23       this week is not going as i had hoped                                                                                                      \n",
       "24       blagh class at 8 tomorrow                                                                                                                  \n",
       "25       I hate when I have to call and wake people up                                                                                              \n",
       "26       Just going to cry myself to sleep after watching Marley and Me.                                                                            \n",
       "27       im sad now  Miss.Lilly                                                                                                                     \n",
       "28       ooooh.... LOL  that leslie.... and ok I won't do it again so leslie won't  get mad again                                                   \n",
       "29       Meh... Almost Lover is the exception... this track gets me depressed every time.                                                           \n",
       "...                                                                                    ...                                                          \n",
       "1599970  Thanks @eastwestchic &amp; @wangyip Thanks! That was just what I was looking for                                                           \n",
       "1599971  @marttn thanks Martin. not the most imaginative interface, but it'll do for now                                                            \n",
       "1599972  @MikeJonesPhoto Congrats Mike  Way to go!                                                                                                  \n",
       "1599973  http://twitpic.com/7jp4n - OMG! Office Space... I wanna steal it.                                                                          \n",
       "1599974  @yrclndstnlvr ahaha nooo you were just away from everyone else! i had to see Kara, she'd die. and yess we aree, ill see you saturday       \n",
       "1599975  @BizCoachDeb  Hey, I'm baack! And, thanks so much for all those kind notes while I was gone. They made me smile at times when I needed it! \n",
       "1599976  @mattycus Yeah, my conscience would be clear in that case.                                                                                 \n",
       "1599977  @MayorDorisWolfe Thats my girl - dishing out the &quot;advice&quot;                                                                        \n",
       "1599978  @shebbs123 i second that                                                                                                                   \n",
       "1599979  In the garden                                                                                                                              \n",
       "1599980  @myheartandmind jo jen by nemuselo zrovna tÃ© holce ael co nic                                                                             \n",
       "1599981  Another Commenting Contest! [;: Yay!!!  http://tinyurl.com/m6j2an                                                                          \n",
       "1599982  @thrillmesoon i figured out how to see my tweets and facebook status updates, and i was set  the groups seemed like a pain to set up...    \n",
       "1599983  @oxhot theri tomorrow, drinking coffee, talking about our most important and favourite issue! YOU know what I mean                         \n",
       "1599984  You heard it here first -- We're having a girl. Hope it has my looks and Wendy's brains. (Kidding, babe).                                  \n",
       "1599985  if ur the lead singer in a band, beware falling prey to LSD &quot;Lead Singer Disease&quot; http://tinyurl.com/n65xjt  #music #haveyouever?\n",
       "1599986  @tarayqueen too much ads on my blog.                                                                                                       \n",
       "1599987  @La_r_a NEVEER  I think that you both will get on well with each other...                                                                  \n",
       "1599988  @Roy_Everitt ha- good job. that's right - we gotta throw that #bigrun tag EVERYWHERE! I wanna get it trending before I start               \n",
       "1599989  @Ms_Hip_Hop im glad ur doing well                                                                                                          \n",
       "1599990  WOOOOO! Xbox is back                                                                                                                       \n",
       "1599991  @rmedina @LaTati Mmmm  That sounds absolutely perfect... but my schedule is full. I won't have time to lay in bed until Sunday. Ugh!!      \n",
       "1599992  ReCoVeRiNg FrOm ThE lOnG wEeKeNd                                                                                                           \n",
       "1599993  @SCOOBY_GRITBOYS                                                                                                                           \n",
       "1599994  @Cliff_Forster Yeah, that does work better than just waiting for it  In the end I just wonder if I have time to keep up a good blog.       \n",
       "1599995  Just woke up. Having no school is the best feeling ever                                                                                    \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta                                                             \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me for details                                                                                   \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur                                                                           \n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H                                                                              \n",
       "\n",
       "[1600000 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Positive']\n",
      "1600000\n"
     ]
    }
   ],
   "source": [
    "## Manually convert sentiment scores to positive/negative binary categories and move to target array\n",
    "\n",
    "X = data.copy()\n",
    "y = [\"Positive\" if sentiment == 4 else \"Negative\" for sentiment in X[\"Sentiment Score\"]]\n",
    "\n",
    "#Since sentiment can be considered to be ordinal, use label encoder\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "\n",
    "X = X.drop(\"Sentiment Score\", axis=1)\n",
    "display(X)\n",
    "display(y)\n",
    "print(le.classes_)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644091</th>\n",
       "      <td>God I'm sort of annoying its like great+ awful+fabulous at the same time!  hahahahah shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721723</th>\n",
       "      <td>In VT, missing my hubby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057340</th>\n",
       "      <td>just got a present from apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628980</th>\n",
       "      <td>@martinsays I want the new album â¥kinda of impossible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172036</th>\n",
       "      <td>@LizzieGrubman have a great time in the Hamptons. Hope it's for some relaxation, not for work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298089</th>\n",
       "      <td>@bunnydozer Not lazy, missy. Actually been crazy busy and they're only open during certain hours. Plus they're 30min away from me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391122</th>\n",
       "      <td>Hangovers are no fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499323</th>\n",
       "      <td>@jennettemccurdy watching a sad episode of One Tree Hill  on the other hand I hear you've been nominated for Best TV Sidekick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70209</th>\n",
       "      <td>@kinagrannis kina! http://tinyurl.com/ct546v why no Uke??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329322</th>\n",
       "      <td>@shani_texas its almost over girl! I'm feeling giddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217173</th>\n",
       "      <td>&amp;quot;He was taking it for a long time, then it started getting to him. It was getting to Dale Jr.&amp;quot; http://snipr.com/j3rsn - Tony Sr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807476</th>\n",
       "      <td>i have just been woken up grrr was having a nice dream too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392455</th>\n",
       "      <td>@a_liss_a I rate you my most interesting person I follow!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718830</th>\n",
       "      <td>well, i have to go to work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173319</th>\n",
       "      <td>@sugabear906 Morning hun! How are you? Love the necklace, it's so cute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124004</th>\n",
       "      <td>maths exam at 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394932</th>\n",
       "      <td>i am so absolutely exhausted. that one hour of sleep did me no justice. oh well. time for breakfast, then the creation museum!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121473</th>\n",
       "      <td>@RhapsodyInBleh Misses you! I wish you could feed me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440725</th>\n",
       "      <td>leaving the beach tomorrow morning. i'm glad i'm going home!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422393</th>\n",
       "      <td>@syifachipuy wahh sayang bgt yaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199447</th>\n",
       "      <td>Yay for being in the showroom all day and not getting to have fun last night  I'm so upset I just used an emoticon. Yay three jobs!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506483</th>\n",
       "      <td>Im HOME!!! and missing the mountains already...  but civilization is very nice too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386767</th>\n",
       "      <td>On the plane flying higher to jump!  http://short.to/dlnc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287577</th>\n",
       "      <td>@HettiSpaghetti Dont make me jealous, it was so hot in the office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194141</th>\n",
       "      <td>regrets losing touch with her highschool best friends. Timothy and Crisselle, I miss you  http://plurk.com/p/x63yf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467338</th>\n",
       "      <td>@lissyvz oh ok!! that's what i was askin...already took pill. wondered how long til i could have breakfast...1/2 hr. is much better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128330</th>\n",
       "      <td>Jackie Brambles is increasingly becoming my favourite loose woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345774</th>\n",
       "      <td>@Annjj fuck me he will love it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425258</th>\n",
       "      <td>I want to talk to jessica or ashlee simpson  please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444301</th>\n",
       "      <td>While I'm at it, I also want a Fluminense shirt.  I think a weekend away to get one is out of the question though.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470485</th>\n",
       "      <td>@BobMaher Having read War and Peace, I would be inclined to argue that it was written by 199 yammering monkeys.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396025</th>\n",
       "      <td>Thank you everyone for the follwing!!! Means so much to us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184779</th>\n",
       "      <td>@RobinTaylorRoth 2,000,000 sq. ft building now totally empty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262752</th>\n",
       "      <td>http://bit.ly/h7Gtc  The real meaning of twitter, a must read  WE are one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284372</th>\n",
       "      <td>Good morning, (it is for ten more minutes anyway!) Exercised;ellipitical trainer +weights,feeling slightly more cheerful, sun is shining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103355</th>\n",
       "      <td>@chrisluvssixxam miss ya already...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791743</th>\n",
       "      <td>is having a bad day already.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247617</th>\n",
       "      <td>@BeaWise can't wait to hear!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327069</th>\n",
       "      <td>Cramped up towards the end of my run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370455</th>\n",
       "      <td>Saw her graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787201</th>\n",
       "      <td>ahh. bimbo..what the - is she trying to do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113396</th>\n",
       "      <td>Finally decided with my teacher that the Masterclass will take place next school year. Sorry to my classmates! Going to cook salmon now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329365</th>\n",
       "      <td>Frikin nervous for science exam tomorrow, it has 175 QUESTIONS!!!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41090</th>\n",
       "      <td>im in work AGAIN cant wait to go home , still feel funny from my migraine i had last night !!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278167</th>\n",
       "      <td>ok back to work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239911</th>\n",
       "      <td>learned some actionscript (3) over the weekend. might start doing more of that flash stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175203</th>\n",
       "      <td>Greg Pritchard was robbed  ii am too gutted for words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912756</th>\n",
       "      <td>Btw; happy mothers day! This week its all about my momma; mothersday dinnner-birthday dinner-surprise bday party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136074</th>\n",
       "      <td>I think it's time to go to bed  Night Night Everyone ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570006</th>\n",
       "      <td>@PreternaReviews yeah, but, dude, that's the key here. I'm a CHICK. Chicks wear pink. Especially the ultra cool rocker chicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999890</th>\n",
       "      <td>TF2 has updated but its way to late to play.  looks like I'll be spying and sniping my heart out tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137337</th>\n",
       "      <td>Last free travel at AP-1  http://yfrog.com/0uvu5j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103462</th>\n",
       "      <td>@cloecouturier Yes, I do have a few ..   4 Girls ...   You son is 23 ... all grown up now .. happens fast!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732180</th>\n",
       "      <td>Thunderstorms yesterday, more on the way. Looks like I won't be online much again today.  HAPPY FATHER'S DAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110268</th>\n",
       "      <td>im starting my many hours of work now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>this song's middle change just doesn't want to be born..... arghhhh!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414414</th>\n",
       "      <td>@officialnjonas Good luck with that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>@ProudGamerTweet I rather average 32370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671155</th>\n",
       "      <td>Pickin up @misstinayao waitin on @sadittysash 2 hurry up...I odeeee missed dem  Table talk 2nite...LOL bout to be fat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>@ home studying for maths wooot ! im so going to fail this shit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                               Tweet\n",
       "644091   God I'm sort of annoying its like great+ awful+fabulous at the same time!  hahahahah shit                                                  \n",
       "721723   In VT, missing my hubby                                                                                                                    \n",
       "1057340  just got a present from apple                                                                                                              \n",
       "628980   @martinsays I want the new album â¥kinda of impossible                                                                                    \n",
       "172036   @LizzieGrubman have a great time in the Hamptons. Hope it's for some relaxation, not for work                                              \n",
       "298089   @bunnydozer Not lazy, missy. Actually been crazy busy and they're only open during certain hours. Plus they're 30min away from me          \n",
       "391122   Hangovers are no fun                                                                                                                       \n",
       "499323   @jennettemccurdy watching a sad episode of One Tree Hill  on the other hand I hear you've been nominated for Best TV Sidekick              \n",
       "70209    @kinagrannis kina! http://tinyurl.com/ct546v why no Uke??                                                                                  \n",
       "1329322  @shani_texas its almost over girl! I'm feeling giddy                                                                                       \n",
       "217173   &quot;He was taking it for a long time, then it started getting to him. It was getting to Dale Jr.&quot; http://snipr.com/j3rsn - Tony Sr. \n",
       "807476   i have just been woken up grrr was having a nice dream too                                                                                 \n",
       "1392455  @a_liss_a I rate you my most interesting person I follow!!                                                                                 \n",
       "718830   well, i have to go to work                                                                                                                 \n",
       "1173319  @sugabear906 Morning hun! How are you? Love the necklace, it's so cute                                                                     \n",
       "124004   maths exam at 1                                                                                                                            \n",
       "1394932  i am so absolutely exhausted. that one hour of sleep did me no justice. oh well. time for breakfast, then the creation museum!             \n",
       "121473   @RhapsodyInBleh Misses you! I wish you could feed me                                                                                       \n",
       "1440725  leaving the beach tomorrow morning. i'm glad i'm going home!                                                                               \n",
       "422393   @syifachipuy wahh sayang bgt yaa                                                                                                           \n",
       "199447   Yay for being in the showroom all day and not getting to have fun last night  I'm so upset I just used an emoticon. Yay three jobs!        \n",
       "1506483  Im HOME!!! and missing the mountains already...  but civilization is very nice too.                                                        \n",
       "1386767  On the plane flying higher to jump!  http://short.to/dlnc                                                                                  \n",
       "287577   @HettiSpaghetti Dont make me jealous, it was so hot in the office                                                                          \n",
       "194141   regrets losing touch with her highschool best friends. Timothy and Crisselle, I miss you  http://plurk.com/p/x63yf                         \n",
       "1467338  @lissyvz oh ok!! that's what i was askin...already took pill. wondered how long til i could have breakfast...1/2 hr. is much better        \n",
       "128330   Jackie Brambles is increasingly becoming my favourite loose woman                                                                          \n",
       "1345774  @Annjj fuck me he will love it                                                                                                             \n",
       "1425258  I want to talk to jessica or ashlee simpson  please                                                                                        \n",
       "444301   While I'm at it, I also want a Fluminense shirt.  I think a weekend away to get one is out of the question though.                         \n",
       "...                                                                                                                       ...                       \n",
       "1470485  @BobMaher Having read War and Peace, I would be inclined to argue that it was written by 199 yammering monkeys.                            \n",
       "1396025  Thank you everyone for the follwing!!! Means so much to us                                                                                 \n",
       "184779   @RobinTaylorRoth 2,000,000 sq. ft building now totally empty.                                                                              \n",
       "1262752  http://bit.ly/h7Gtc  The real meaning of twitter, a must read  WE are one.                                                                 \n",
       "1284372  Good morning, (it is for ten more minutes anyway!) Exercised;ellipitical trainer +weights,feeling slightly more cheerful, sun is shining   \n",
       "103355   @chrisluvssixxam miss ya already...                                                                                                        \n",
       "791743   is having a bad day already.                                                                                                               \n",
       "1247617  @BeaWise can't wait to hear!                                                                                                               \n",
       "327069   Cramped up towards the end of my run                                                                                                       \n",
       "1370455  Saw her graduate                                                                                                                           \n",
       "787201   ahh. bimbo..what the - is she trying to do                                                                                                 \n",
       "1113396  Finally decided with my teacher that the Masterclass will take place next school year. Sorry to my classmates! Going to cook salmon now    \n",
       "329365   Frikin nervous for science exam tomorrow, it has 175 QUESTIONS!!!!!!!!!!                                                                   \n",
       "41090    im in work AGAIN cant wait to go home , still feel funny from my migraine i had last night !!                                              \n",
       "278167   ok back to work                                                                                                                            \n",
       "1239911  learned some actionscript (3) over the weekend. might start doing more of that flash stuff                                                 \n",
       "175203   Greg Pritchard was robbed  ii am too gutted for words                                                                                      \n",
       "912756   Btw; happy mothers day! This week its all about my momma; mothersday dinnner-birthday dinner-surprise bday party                           \n",
       "1136074  I think it's time to go to bed  Night Night Everyone ?                                                                                     \n",
       "1570006  @PreternaReviews yeah, but, dude, that's the key here. I'm a CHICK. Chicks wear pink. Especially the ultra cool rocker chicks              \n",
       "999890   TF2 has updated but its way to late to play.  looks like I'll be spying and sniping my heart out tomorrow                                  \n",
       "137337   Last free travel at AP-1  http://yfrog.com/0uvu5j                                                                                          \n",
       "1103462  @cloecouturier Yes, I do have a few ..   4 Girls ...   You son is 23 ... all grown up now .. happens fast!!                                \n",
       "732180   Thunderstorms yesterday, more on the way. Looks like I won't be online much again today.  HAPPY FATHER'S DAY                               \n",
       "110268   im starting my many hours of work now                                                                                                      \n",
       "259178   this song's middle change just doesn't want to be born..... arghhhh!!                                                                      \n",
       "1414414  @officialnjonas Good luck with that                                                                                                        \n",
       "131932   @ProudGamerTweet I rather average 32370                                                                                                    \n",
       "671155   Pickin up @misstinayao waitin on @sadittysash 2 hurry up...I odeeee missed dem  Table talk 2nite...LOL bout to be fat...                   \n",
       "121958   @ home studying for maths wooot ! im so going to fail this shit                                                                            \n",
       "\n",
       "[160000 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "160000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Split data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "#display(X_train)\n",
    "\n",
    "display(X_train)\n",
    "#display(X_valid)\n",
    "display(y_train)\n",
    "display(len(y_train))\n",
    "#display(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_valid_2, y_train_2, y_valid_2 = train_test_split(X, y, test_size=0.9, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"anova_filter = SelectKBest(f_regression, k=5)\\nclf = svm.SVC(kernel='linear')\\nanova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])\\n# You can set the parameters using the names issued\\n# For instance, fit using a k of 10 in the SelectKBest\\n# and a parameter 'C' of the svm\\nanova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)\\n                      \\nPipeline(memory=None,\\n         steps=[('anova', SelectKBest(...)),\\n                ('svc', SVC(...))])\\nprediction = anova_svm.predict(X)\\n#anova_svm.score(X, y)                        \\n#0.83\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example \n",
    "# ANOVA SVM-C\n",
    "'''anova_filter = SelectKBest(f_regression, k=5)\n",
    "clf = svm.SVC(kernel='linear')\n",
    "anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])\n",
    "# You can set the parameters using the names issued\n",
    "# For instance, fit using a k of 10 in the SelectKBest\n",
    "# and a parameter 'C' of the svm\n",
    "anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)\n",
    "                      \n",
    "Pipeline(memory=None,\n",
    "         steps=[('anova', SelectKBest(...)),\n",
    "                ('svc', SVC(...))])\n",
    "prediction = anova_svm.predict(X)\n",
    "#anova_svm.score(X, y)                        \n",
    "#0.83'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create preprocessing pipelines\n",
    "\n",
    "## Column selector (removes non-essential columns as defined by user), X must be a pandas dataframe\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "        try:\n",
    "            return X[self.columns]\n",
    "        except KeyError:\n",
    "            cols_error = list(set(self.columns) - set(X.columns))\n",
    "            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom transforms and preprocessing pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numeric features if applicable\n",
    "#numeric_features = ['age', 'fare']\n",
    "#numeric_transformer = Pipeline(steps=[\n",
    "#    ('imputer', SimpleImputer(strategy='median')),\n",
    "#    ('scaler', StandardScaler())])\n",
    "\n",
    "## Categorical features if applicable\n",
    "#categorical_features = [\"Sentiment Score\"]\n",
    "#categorical_transformer = Pipeline(steps=[\n",
    "#    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "#preprocessor = ColumnTransformer(\n",
    "#    transformers=[\n",
    "#        ('num', numeric_transformer, numeric_features),\n",
    "#        (\"cat\", categorical_transformer, categorical_features)])\n",
    "\n",
    "## Create custom transformer\n",
    "class Tweet_cleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Cleans tweets by removing urls\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.verbose:\n",
    "            print(\"Verbose mode on!\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        #display(X)\n",
    "        #Use beautifulsoup to decode HTML code\n",
    "        if self.verbose:\n",
    "            print(\"Preprocessing...\")\n",
    "            \n",
    "        X_1 = [(BeautifulSoup(tweet,\"lxml\").get_text()) for tweet in X[\"Tweet\"]]\n",
    "        #X_2 = [re.sub(r\"@[A-Za-z0-9]+\",\"\",tweet) for tweet in X_1]\n",
    "        X_3 = [re.sub(\"https?://[A-Za-z0-9./]+\",\"\",tweet) for tweet in X_1]\n",
    "        X_4 = [re.sub(\"www.[A-Za-z0-9./]+\",\"\",tweet) for tweet in X_3]\n",
    "        #X_5 = [(tweet.decode(\"utf-8-sig\")).replace(u\"\\ufffd\",\"\") for tweet in X_4]\n",
    "        #X_5 = [re.sub(\"[^a-zA-Z]\",\" \", tweet) for tweet in X_4]\n",
    "        \n",
    "        if self.verbose:\n",
    "        #display(X_4)\n",
    "            print(\"Preprocess Complete\")\n",
    "\n",
    "        return X_4\n",
    "    \n",
    "\n",
    "class Keras_tokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, verbose=False, path=None, load_tokenizer=True, nb_words):\n",
    "        self.verbose = verbose\n",
    "        self.path = path\n",
    "        self.nb_words = nb_words\n",
    "        self.load_tokenizer = load_tokenizer\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.verbose:\n",
    "            print(\"Verbose mode on!\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        #display(X)\n",
    "        ## Use Keras tokenizer and padding for neural net inputs\n",
    "        if self.verbose:\n",
    "            print(\"Tokenizing...\")\n",
    "            \n",
    "        if self.load_tokenizer:\n",
    "            with open(os.path.join(self.path, \"keras_tokenizer.pkl\"), \"rb\") as handle:\n",
    "                tokenizer = pickle.load(handle)\n",
    "        else:\n",
    "            tokenizer = Tokenizer(nb_words=self.nb_words,lower=True,split=' ')\n",
    "            tokenizer.fit_on_texts(X[\"Tweet\"].tolist())\n",
    "            #print(tokenizer.word_index)  # To see the dictionary\n",
    "            with open(os.path.join(self.path, \"keras_tokenizer.pkl\"), \"wb\") as handle:\n",
    "                pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "        encoded_sentences = tokenizer.texts_to_sequences(X[\"Tweet\"].tolist())\n",
    "        padded_sentences = pad_sequences(X[\"Tweet\"].tolist(), maxlen=50, padding=\"post\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            display(padded_sentences[:50])\n",
    "            print(\"Tokenization Complete\")\n",
    "\n",
    "        return padded_sentences\n",
    "    \n",
    "    \n",
    "class Gensim_doc2vec(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if(self.verbose):\n",
    "            print(\"Verbose mode on!\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        ## Train gensim doc2vec embeddings\n",
    "        tokenizer = Tokenizer(nb_words=2500,lower=True,split=' ')\n",
    "        tokenizer.fit_on_texts(X[\"Tweet\"].tolist())\n",
    "        #print(tokenizer.word_index)  # To see the dictionary\n",
    "        with open(\"keras_tokenizer.pkl\", \"wb\") as handle:\n",
    "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "        encoded_sentences = tokenizer.texts_to_sequences(X[\"Tweet\"].tolist())\n",
    "        padded_sentences = pad_sequences(X[\"Tweet\"].tolist(),maxlen=50,padding=\"post\")\n",
    "        display(padded_sentences)\n",
    "        \n",
    "        print(\"Tokenization Complete\")\n",
    "\n",
    "        return padded_sentences\n",
    "    \n",
    "    \n",
    "## Categorical features if applicable\n",
    "tweet_col = [\"Tweet\"]\n",
    "tweet_transformer_pipe = Pipeline(steps=[\n",
    "    (\"cleaner\", tweet_cleaner()),\n",
    "    (\"tfidf_vectorizer\", TfidfVectorizer())\n",
    "    ])\n",
    "\n",
    "#tweet_transformer_keras = Pipeline(steps=[\n",
    "#    (\"cleaner\", tweet_cleaner()),\n",
    "#    (\"keras_tokenizer\", keras_tokenizer())\n",
    "#    ])\n",
    "                      \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tweet_transformer\", tweet_transformer_pipe, tweet_col)\n",
    "        ])\n",
    "\n",
    "preprocessor_keras = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tweet_transformer_keras\", tweet_transformer_keras, tweet_col)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w...e))]), ['Tweet'])])), ('nb_classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w...penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=2, warm_start=False))])\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w...obs=None,\n",
      "            oob_score=False, random_state=None, verbose=2,\n",
      "            warm_start=False))])\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w...    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=2, warm_start=False))])\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w..., reg_lambda=0.0, silent=False,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0))])\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w...lpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1, verbosity=1))])\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w...='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=2))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w...'])])), ('lstm_classifier', <keras.wrappers.scikit_learn.KerasClassifier object at 0x7f2a48d11da0>)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfrom numpy import array\\nfrom numpy import asarray\\nfrom numpy import zeros\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.layers import Flatten\\nfrom keras.layers import Embedding\\n# define documents\\ndocs = ['Well done!',\\n\\t\\t'Good work',\\n\\t\\t'Great effort',\\n\\t\\t'nice work',\\n\\t\\t'Excellent!',\\n\\t\\t'Weak',\\n\\t\\t'Poor effort!',\\n\\t\\t'not good',\\n\\t\\t'poor work',\\n\\t\\t'Could have done better.']\\n# define class labels\\nlabels = array([1,1,1,1,1,0,0,0,0,0])\\n# prepare tokenizer\\nt = Tokenizer()\\nt.fit_on_texts(docs)\\nvocab_size = len(t.word_index) + 1\\n# integer encode the documents\\nencoded_docs = t.texts_to_sequences(docs)\\nprint(encoded_docs)\\n# pad documents to a max length of 4 words\\nmax_length = 4\\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\\nprint(padded_docs)\\n# load the whole embedding into memory\\nembeddings_index = dict()\\nf = open('../glove_data/glove.6B/glove.6B.100d.txt')\\nfor line in f:\\n\\tvalues = line.split()\\n\\tword = values[0]\\n\\tcoefs = asarray(values[1:], dtype='float32')\\n\\tembeddings_index[word] = coefs\\nf.close()\\nprint('Loaded %s word vectors.' % len(embeddings_index))\\n# create a weight matrix for words in training docs\\nembedding_matrix = zeros((vocab_size, 100))\\nfor word, i in t.word_index.items():\\n\\tembedding_vector = embeddings_index.get(word)\\n\\tif embedding_vector is not None:\\n\\t\\tembedding_matrix[i] = embedding_vector\\n# define model\\nmodel = Sequential()\\ne = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\\nmodel.add(e)\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation='sigmoid'))\\n# compile the model\\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\\n# summarize the model\\nprint(model.summary())\\n# fit the model\\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\\n# evaluate the model\\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\\nprint('Accuracy: %f' % (accuracy*100))\\n\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initialize classifiers\n",
    "\n",
    "## Long short-term memory neural network\n",
    "def Create_LSTM(embed_dim=128,lstm_out=200):#,dropout=0.2,dropout_U=0.2,dropout_W=0.2):\n",
    "    #embed_dim = 128\n",
    "    #lstm_out = 200\n",
    "    #dropout = 0.2\n",
    "    #dropout_U = 0.2\n",
    "    #dropout_W = 0.2\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=40,output_dim=embed_dim))#,dropout=dropout))\n",
    "    model.add(layers.LSTM(lstm_out))#,dropout_U=dropout_U,dropout_W=dropout_W))\n",
    "    model.add(layers.Dense(2,activation=\"softmax\"))\n",
    "    model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "## Multinomial Naive Bayes\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf_fullpipe = Pipeline(steps=[\n",
    "                            (\"preprocessor\", preprocessor),\n",
    "                            (\"nb_classifier\", nb_clf)\n",
    "                            ])\n",
    "print(nb_clf_fullpipe)\n",
    "print(\"\")\n",
    "\n",
    "## Logistic Regression\n",
    "lr_clf = LogisticRegression(verbose=2)\n",
    "lr_clf_fullpipe = Pipeline(steps=[\n",
    "                            (\"preprocessor\", preprocessor),\n",
    "                            (\"lr_classifier\", lr_clf)\n",
    "                            ])\n",
    "print(lr_clf_fullpipe)\n",
    "print(\"\")\n",
    "\n",
    "## Random Forest\n",
    "rf_clf = RandomForestClassifier(verbose=2)\n",
    "rf_clf_fullpipe = Pipeline(steps=[\n",
    "                            (\"preprocessor\", preprocessor),\n",
    "                            (\"rf_classifier\", rf_clf)\n",
    "                            ])\n",
    "print(rf_clf_fullpipe)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "## Gradient Boost\n",
    "gb_clf = GradientBoostingClassifier(verbose=2)\n",
    "gb_clf_fullpipe = Pipeline(steps=[\n",
    "                            (\"preprocessor\", preprocessor),\n",
    "                            (\"gb_classifier\", gb_clf)\n",
    "                            ])\n",
    "print(gb_clf_fullpipe)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "## LightGBM\n",
    "lgbm_clf = LGBMClassifier(silent=False,n_jobs=1)\n",
    "lgbm_clf_fullpipe = Pipeline(steps=[\n",
    "                                (\"preprocessor\", preprocessor),\n",
    "                                (\"lgbm_classifier\", lgbm_clf)\n",
    "                            ])\n",
    "print(lgbm_clf_fullpipe)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "## XGBoost\n",
    "xgb_clf = XGBClassifier(verbosity=1)\n",
    "xgb_clf_fullpipe = Pipeline(steps=[\n",
    "                                (\"preprocessor\", preprocessor),\n",
    "                                (\"xgb_classifier\", xgb_clf)\n",
    "                            ])\n",
    "print(xgb_clf_fullpipe)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "## Support Vector Classification\n",
    "svm_clf = SVC(verbose=2)\n",
    "svm_clf_fullpipe = Pipeline(steps=[\n",
    "                                (\"preprocessor\", preprocessor),\n",
    "                                (\"svm_classifier\", svm_clf)\n",
    "                            ])\n",
    "print(svm_clf_fullpipe)\n",
    "\n",
    "# wrap the model using the function you created\n",
    "LSTM_clf = KerasClassifier(build_fn=create_LSTM,verbose=2)\n",
    "\n",
    "LSTM_clf_fullpipe = Pipeline(steps=[\n",
    "                                (\"preprocessor\", preprocessor),\n",
    "                                (\"lstm_classifier\", LSTM_clf)\n",
    "                            ])\n",
    "print(LSTM_clf_fullpipe)\n",
    "\n",
    "'''\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('../glove_data/glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector\n",
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiate a full preprocessing and prediction pipeline\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(10,110,11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "gamma = [x for x in np.logspace(-15,3,base=2,num=7)]\n",
    "gamma.append(\"scale\")\n",
    "gamma.append(\"auto\")\n",
    "\n",
    "## RandomizedSearchCV hyperparameter optimization\n",
    "nb_param_grid = {\n",
    "    \"nb_classifier__alpha\": [1,1e-1,1e-2,0]\n",
    "    }\n",
    "\n",
    "lr_param_grid = {\n",
    "    \"lr_classifier__penalty\": [\"l1\",\"l2\"],\n",
    "    \"lr_classifier__C\": [1e-3,1e-2,1e-1,1,10]\n",
    "    }\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"rf_classifier__n_estimators\": [int(x) for x in np.linspace(50,500,11)],\n",
    "    \"rf_classifier__max_depth\": max_depth,\n",
    "    \"rf_classifier__max_features\": [\"auto\"],\n",
    "    \"rf_classifier__min_samples_split\": [2, 10, 100],\n",
    "    \"rf_classifier__min_samples_leaf\": [1, 2, 4, 10],\n",
    "    }\n",
    "\n",
    "gb_param_grid = {\n",
    "    \"gb_classifier__n_estimators\": [int(x) for x in np.linspace(50,500,11)],\n",
    "    \"gb_classifier__max_depth\": max_depth\n",
    "    }\n",
    "\n",
    "lgbm_param_grid = {\n",
    "    \"lgbm_classifier__n_estimators\": [int(x) for x in np.linspace(50,550,11)],\n",
    "    \"lgbm_classifier__max_depth\": max_depth,\n",
    "    }\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"xgb_classifier__n_estimators\": [int(x) for x in np.linspace(50,550,11)],\n",
    "    \"xgb_classifier__max_depth\": max_depth,  \n",
    "    }\n",
    "\n",
    "svm_param_grid = {\n",
    "    \"svm_classifier__C\": [x for x in np.logspace(-5,15,base=2,num=7)],\n",
    "    \"svm_classifier__gamma\": gamma\n",
    "    }\n",
    "\n",
    "lstm_param_grid = {\n",
    "    \"lstm_classifier__embed_dim\": [128],\n",
    "    \"lstm_classifier__lstm_out\": [200],\n",
    "    \"lstm_classifier__dropout\": [0.2],\n",
    "    \"lstm_classifier__dropout_U\": [0.2],\n",
    "    \"lstm_classifier__dropout_W\": [0.2],\n",
    "    \"lstm_classifier__epochs\": [50, 100, 150],\n",
    "    \"lstm_classifier__batch_size\": [5, 10, 20]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    #\"preprocessor__tweet_transformer__tfidf_vectorizer__max_df\": np.linspace(0.2, 1, 10),\n",
    "    \"preprocessor__tweet_transformer__tfidf_vectorizer__binary\": [True],\n",
    "    \"preprocessor__tweet_transformer__tfidf_vectorizer__token_pattern\": [r\"(?u)\\b\\w\\w+\\b|\\'\"],\n",
    "    \"preprocessor__tweet_transformer__tfidf_vectorizer__ngram_range\": [(1,2)],\n",
    "    }\n",
    "\n",
    "nb_param_grid.update(param_grid)\n",
    "lr_param_grid.update(param_grid)\n",
    "rf_param_grid.update(param_grid)\n",
    "gb_param_grid.update(param_grid)\n",
    "lgbm_param_grid.update(param_grid)\n",
    "xgb_param_grid.update(param_grid)\n",
    "svm_param_grid.update(param_grid)\n",
    "lstm_param_grid.update(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of pipelines and classifier types for ease of reference\n",
    "pipe_dict = {\n",
    "            \"MultinomialNB\" : (nb_clf_fullpipe, nb_param_grid),\n",
    "            \"LogisticRegression\" : (lr_clf_fullpipe, lr_param_grid),\n",
    "            \"RandomForest\" : (rf_clf_fullpipe, rf_param_grid),\n",
    "            \"GradientBoosting\" : (gb_clf_fullpipe, gb_param_grid),\n",
    "            \"XGBoost\" : (xgb_clf_fullpipe, xgb_param_grid),\n",
    "            \"LightGBM\" : (lgbm_clf_fullpipe, lgbm_param_grid),\n",
    "            \"SVC\" : (svm_clf_fullpipe, svm_param_grid),\n",
    "            \"LSTM\" : (LSTM_clf_fullpipe, lstm_param_grid)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to fit a model pipeline\n",
    "def Pipeline_fit(pipeline, search_method, cv, iid, verbose_searchcv, n_jobs, optimization):\n",
    "\n",
    "    if optimization == True:\n",
    "        ## Do hyperparameter optimization using RandomizedSearchCV\n",
    "        \n",
    "        if search_method == \"randomsearch\":\n",
    "            search_cv = RandomizedSearchCV(pipe_dict[pipeline][0], pipe_dict[pipeline][1], \n",
    "                                           cv=cv, iid=iid, n_jobs=n_jobs)#verbose=verbose_searchcv, n_jobs=n_jobs)\n",
    "        elif search_method == \"gridsearch\":\n",
    "            search_cv = GridSearchCV(pipe_dict[pipeline][0], pipe_dict[pipeline][1], \n",
    "                                     cv=cv, iid=iid, n_jobs=n_jobs)#verbose=verbose_searchcv, n_jobs=n_jobs)\n",
    "            \n",
    "        ## Train the model\n",
    "        print(\"Training \"+pipeline+\" on \"+str(len(y_train))+\" samples...\")\n",
    "        search_cv.fit(X_train, y_train)\n",
    "\n",
    "        print(\"\")\n",
    "        print(pipeline+\":\")\n",
    "        print(\"Best Score: \", search_cv.best_score_)\n",
    "        print(\"Best Params: \", search_cv.best_params_)\n",
    "\n",
    "        ## Use joblib to dump the best estimator to joblib file for persistence purposes\n",
    "        clf_final = search_cv.best_estimator_\n",
    "        dump(clf_final, \"twitter_\"+pipeline+\".joblib\")\n",
    "\n",
    "        return clf_final\n",
    "    \n",
    "    else:\n",
    "        ## No optimization is performed, fit to model only\n",
    "        print(\"Training \"+pipeline+\" on \"+str(len(y_train))+\" samples...\")\n",
    "        print(pipe_dict[pipeline][0].steps[1])\n",
    "        clf_final = pipe_dict[pipeline][0].fit(X_train, y_train)\n",
    "\n",
    "        ## Use joblib to dump the best estimator to joblib file for persistence purposes\n",
    "        dump(clf_final, \"twitter_\"+pipeline+\".joblib\")\n",
    "\n",
    "        return clf_final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Activate full pipeline\n",
    "\"\"\"\n",
    "lgbm_clf_final = pipeline_fit(pipeline=\"LightGBM\", \n",
    "                                search_method=\"randomsearch\",\n",
    "                                cv=5, \n",
    "                                iid=False, \n",
    "                                verbose_searchcv=1, \n",
    "                                n_jobs=1, \n",
    "                                optimization=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nb_clf_final = pipeline_fit(pipeline=\"MultinomialNB\", \n",
    "                            search_method=\"gridsearch\",\n",
    "                            cv=5, \n",
    "                            iid=False,\n",
    "                            verbose_searchcv=1, \n",
    "                            n_jobs=1, \n",
    "                            optimization=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression on 160000 samples...\n",
      "Preprocessing...\n",
      "Preprocess Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Michael/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "Preprocessing...\n",
      "Preprocess Complete\n",
      "[LibLinear]\n",
      "LogisticRegression:\n",
      "Best Score:  0.7996812500000001\n",
      "Best Params:  {'lr_classifier__C': 10, 'lr_classifier__penalty': 'l2', 'preprocessor__tweet_transformer__tfidf_vectorizer__binary': True, 'preprocessor__tweet_transformer__tfidf_vectorizer__ngram_range': (1, 2), 'preprocessor__tweet_transformer__tfidf_vectorizer__token_pattern': \"(?u)\\\\b\\\\w\\\\w+\\\\b|\\\\'\"}\n"
     ]
    }
   ],
   "source": [
    "lr_clf_final = pipeline_fit(pipeline=\"LogisticRegression\", \n",
    "                            search_method=\"gridsearch\",\n",
    "                            cv=5, \n",
    "                            iid=False, \n",
    "                            verbose_searchcv=2, \n",
    "                            n_jobs=1, \n",
    "                            optimization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "xgb_clf_final = pipeline_fit(pipeline=\"XGBoost\", \n",
    "                            search_method=\"randomsearch\",\n",
    "                            cv=5, \n",
    "                            iid=False, \n",
    "                            verbose_searchcv=1, \n",
    "                            n_jobs=1, \n",
    "                            optimization=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "rf_clf_final = pipeline_fit(pipeline=\"RandomForest\", \n",
    "                            search_method=\"randomsearch\",\n",
    "                            cv=5, \n",
    "                            iid=False, \n",
    "                            verbose_searchcv=1, \n",
    "                            n_jobs=1, \n",
    "                            optimization=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "svm_clf_final = pipeline_fit(pipeline=\"SVC\", \n",
    "                            search_method=\"randomsearch\",\n",
    "                            cv=5, \n",
    "                            iid=False, \n",
    "                            verbose_searchcv=2, \n",
    "                            n_jobs=1, \n",
    "                            optimization=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lstm_clf_final = pipeline_fit(pipeline=\"LSTM\", \n",
    "                            search_method=\"randomsearch\",\n",
    "                            cv=5, \n",
    "                            iid=False, \n",
    "                            verbose_searchcv=2, \n",
    "                            n_jobs=1, \n",
    "                            optimization=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load estimator via joblib\n",
    "\n",
    "#lgbm_clf_final = load(\"twitter_LightGBM.joblib\")\n",
    "rf_clf_final = load(\"twitter_RandomForest.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lgbm_clf_final.named_steps[\"lgbm_classifier\"])\n",
    "print(lgbm_clf_final.named_steps)\n",
    "print(\"\")\n",
    "#print(rf_clf_final.named_steps[\"rf_classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use a soft voting classifier to make final predictions from all models\n",
    "vote_clf = VotingClassifier(estimators=[\n",
    "                            (\"nb_clf\", nb_clf_final.named_steps[\"nb_classifier\"]), \n",
    "                            (\"rf_clf\", rf_clf_final.named_steps[\"rf_classifier\"]),\n",
    "                            #(\"gb_clf\", gb_clf_final), \n",
    "                            #(\"xgb_clf\", xgb_clf_final),\n",
    "                            (\"lgbm_clf\", lgbm_clf_final.named_steps[\"lgbm_classifier\"])\n",
    "                            ], voting=\"soft\")\n",
    "\n",
    "vote_clf_fullpipe = Pipeline(steps=[\n",
    "                                (\"preprocessor\", preprocessor),\n",
    "                                (\"voting_classifier\", vote_clf)\n",
    "                            ])\n",
    "\n",
    "print(vote_clf_fullpipe)\n",
    "\n",
    "\n",
    "pipe_dict[\"VotingClassifier\"] = (vote_clf_fullpipe,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_clf_final = pipeline_fit(pipeline=\"VotingClassifier\", cv=None, iid=False, verbose=2, n_jobs=1, optimization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(X_valid)\n",
    "y_pred = vote_clf_final.predict(X_valid)\n",
    "#y_pred = lgbm_clf_final.predict(X_valid)\n",
    "print(\"Predicted Targets:\",y_pred)\n",
    "print(\"Actual Targets:\",y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = lgbm_clf_final.predict(X_valid_2)\n",
    "print(\"Predicted Targets:\",y_pred_2)\n",
    "print(\"Actual Targets:\",y_valid_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_inv = le.inverse_transform(y_pred)\n",
    "target_names = le.classes_\n",
    "print(\"Target names: \",target_names)\n",
    "n_classes = le.classes_.shape[0]\n",
    "print(\"n_classes: \",n_classes)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred, target_names=target_names))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_valid, y_pred, labels=range(n_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = le.classes_\n",
    "print(\"Target names: \",target_names)\n",
    "n_classes = le.classes_.shape[0]\n",
    "print(\"n_classes: \",n_classes)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid_2, y_pred_2, target_names=target_names))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_valid_2, y_pred_2, labels=range(n_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    if not title:\n",
    "        title = \"Confusion matrix\"\n",
    "        \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    ## Get normalized confusion matrix\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(cm)\n",
    "    print(\"Normalized Confusion Matrix\")\n",
    "    print(cm_norm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.grid(None)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel=\"True Labels\",\n",
    "           xlabel=\"Predicted Labels\"\n",
    "          )\n",
    "\n",
    "    ## Rotate the tick labels and set their alignment.\n",
    "    #plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "    #         rotation_mode=\"anchor\")\n",
    "\n",
    "    ## Loop over data dimensions and create text annotations \n",
    "    ## using both normalized and non-normalized confusion matrices\n",
    "    def annotated_text(i,j):\n",
    "        return str(cm[i,j])+\"\\n(\"+str(format(cm_norm[i,j], \".2f\"))+\")\"\n",
    "    \n",
    "    ## Text color switches to provide contrast\n",
    "    col_switch_thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, annotated_text(i,j),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > col_switch_thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "#np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_valid, y_pred, classes=le.classes_,title=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del lgbm_clf_final\n",
    "\n",
    "## Use mglearn to extract feature/n-gram names and their respective importance coefficients\n",
    "#print(lgbm_clf_final.named_steps[\"preprocessor\"].transformers_[0][1].named_steps[\"tfidf_vectorizer\"])\n",
    "vectors = rf_clf_final.named_steps[\"preprocessor\"].transformers_[0][1].named_steps[\"tfidf_vectorizer\"]\n",
    "feature_names = np.array(vectors.get_feature_names())\n",
    "coeff = rf_clf_final.named_steps[\"rf_classifier\"].feature_importances_\n",
    "mglearn.tools.visualize_coefficients(coeff,feature_names,n_top_features=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
