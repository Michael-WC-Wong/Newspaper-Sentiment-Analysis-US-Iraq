{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Michael/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ver. 3.6.4\n",
      "numpy ver. 1.16.0\n",
      "pandas ver. 0.23.4\n",
      "seaborn ver. 0.9.0\n",
      "matplotlib ver. 2.1.2\n",
      "scikit-learn ver. 0.20.3\n",
      "XGBoost ver. 0.82\n",
      "lightgbm ver. 2.2.3\n",
      "keras ver. 2.2.4\n"
     ]
    }
   ],
   "source": [
    "############### PYTHON 3\n",
    "\n",
    "\n",
    "\n",
    "#Module Imports\n",
    "import platform\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import re\n",
    "import json\n",
    "import lightgbm\n",
    "import keras\n",
    "import xgboost\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from joblib import dump, load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.wrappers.scikit_learn.\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "print(\"Python ver. \"+platform.python_version())\n",
    "print(\"numpy ver. \"+np.__version__)\n",
    "print(\"pandas ver. \"+pd.__version__)\n",
    "print(\"seaborn ver. \"+sns.__version__)\n",
    "print(\"matplotlib ver. \"+matplotlib.__version__)\n",
    "print(\"scikit-learn ver. \"+sklearn.__version__)\n",
    "print(\"XGBoost ver. \"+xgboost.__version__)\n",
    "print(\"lightgbm ver. \"+lightgbm.__version__)\n",
    "print(\"keras ver. \"+keras.__version__)\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment Score</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>Just woke up. Having no school is the best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment Score  \\\n",
       "1599995  4                 \n",
       "1599996  4                 \n",
       "1599997  4                 \n",
       "1599998  4                 \n",
       "1599999  4                 \n",
       "\n",
       "                                                                                  Tweet  \n",
       "1599995  Just woke up. Having no school is the best feeling ever                         \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta  \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me for details                        \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur                \n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H                   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Read-in, ignore unnecessary columns\n",
    "data = pd.read_csv(\"Data/twitter_sentiment_data/training.1600000.processed.noemoticon.csv\",encoding = \"ISO-8859-1\",usecols=[0,5],names=[\"Sentiment Score\",\"Tweet\"])\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Sentiment Score\n",
      "count  1.600000e+06   \n",
      "mean   2.000000e+00   \n",
      "std    2.000001e+00   \n",
      "min    0.000000e+00   \n",
      "25%    0.000000e+00   \n",
      "50%    2.000000e+00   \n",
      "75%    4.000000e+00   \n",
      "max    4.000000e+00   \n",
      "1600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Michael/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1706: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1UAAAG4CAYAAACtus2zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XuQ13Wh//HX7nJTicuuggtaZo22aZ5IwFK8oYl2VrDhGLajHe8/My+lopsmFOmcQE6al7TM8dQZT5ZHRUUTT4Nm6mRYOWJQKoEirIssEBeVy+7n90fTTuSFhY/sCj4eM87s9/v+fL77/r73Ox++Tz+f/W5FURRFAAAA2CKVXT0BAACAbZmoAgAAKEFUAQAAlCCqAAAAShBVAAAAJYgqAACAEkQVwHvYhAkTcsMNN7wrj7V48eIMGTIkra2tSZKTTjopd9xxx7vy2Ely+umn5+67737XHq+jrr766hxwwAE56KCDOv17/7MhQ4Zk4cKFXT0NADpZhb9TBdA1Ro4cmaVLl6aqqipVVVX56Ec/mjFjxmTcuHGprNy8/+c1cuTIXHHFFTnwwAM7vM9JJ52U0aNH5/jjj9/cqee6667Liy++mKlTp272vu+mpqamjBo1Kg8//HBqamrecpubbropP//5z7N8+fJ84AMfyKc+9alcc801pb93mfV7t3Xk57+11gGApFtXTwDg/eymm27KgQcemFWrVuW3v/1trrzyyjzzzDP5j//4j3f1+2zYsCHdum1/h/xFixalX79+bxtUd999d+65557813/9Vz74wQ/m1VdfzcyZMzt5ll2vM9Zhe32NAXSEy/8A3gM+8IEP5Igjjsg111yTu+++O88991ySpLGxMVdffXWSZNmyZfl//+//ZejQoRk+fHgaGhrS1taW8ePHZ/HixTnrrLMyZMiQ3HzzzXn55Zez995754477shhhx2Wf//3f2+/b8OGDe3f96WXXsq//du/Zf/998+Xv/zlrFixIkny5JNP5pBDDtlojiNHjswTTzyRRx99ND/4wQ/yi1/8IkOGDMno0aOTbHw5YVtbW77//e/n8MMPz2c+85lcfPHFWbVqVZK0z+Puu+/OYYcdlgMOOCA33njj267NqlWrcvHFF+fTn/50Dj/88Hz/+99PW1tbnnjiiZx66qlZsmRJhgwZksbGxjftO3v27IwYMSIf/OAHkyS77LJLxo0bt9FjX3rppRkxYkQOPvjgXH311e2XR95111354he/mMmTJ2fYsGEZOXJkfvWrXyX52yWHTz31VCZNmpQhQ4Zk0qRJSZK99947L774YvvP7pvf/GZOP/30DBkyJCeccEJeffXVXHnllRk2bFiOPvrozJkzp30uzc3NOffcc/PpT386I0eOzE9+8pP2seuuuy7nn39+Lr744gwZMiT/+q//mtmzZyfJW/78N3cdVqxYka9//esZMWJEhg0blrPPPrt97Oc//3k++9nPZvjw4TnrrLPS3NzcPrb33nvntttuy1FHHZWjjjoqSTJv3ryccsopGT58eEaNGpUHHnigfftf/epX+dznPpchQ4bk4IMPzi233PK2P3eAbUoBQJc4/PDDi8cff/xN9x966KHFbbfdVhRFUVxyySXFd7/73aIoimLq1KnF5ZdfXqxbt65Yt25dMWvWrKKtre0tH2vhwoXFXnvtVYwfP75Ys2ZN8frrr7fft379+qIoiuLEE08sRowYUfz5z38u1qxZU5xzzjnFhRdeWBRFUfzmN78pDj744Led77XXXtu+7d+deOKJxc9//vOiKIrijjvuKI488sjipZdeKlavXl185StfKS666KKN5nbZZZcVr7/+ejF37txin332KV544YW3XKfx48cXZ511VrFq1api4cKFxVFHHdX+fd5qnv9o2rRpxbBhw4qbb765eOaZZ4oNGzZsNP7lL3+5uPzyy4s1a9YUS5cuLcaOHVv89Kc/LYqiKO68887i4x//ePGzn/2s2LBhQ3HbbbcVBx10UPua/+Pz/bu99tqrWLBgQfvPbvjw4cXs2bOLN954ozjppJOKww8/vLj77ruLDRs2FN/97neLE088sSiKomhtbS0+//nPF9ddd12xdu3a4qWXXipGjhxZPProo+3rve+++xaPPPJIsWHDhmLq1KnF8ccf/5Y/my1ZhzPOOKM4//zzixUrVhTr1q0rnnzyyaIoiuKJJ54ohg8fXjz77LPF2rVri0mTJhUNDQ0bPd+TTz65WL58efH6668Xa9asKQ455JDif//3f4v169cXzz77bDF8+PDiueeeK4qiKA466KBi1qxZRVEUxYoVK4pnn332becMsC1xpgrgPWbAgAH561//+qb7u3XrlldffTWLFy9O9+7dM3To0FRUVLzjY5177rnZcccd06tXr7ccHzNmTPbaa6/suOOOOf/88/Pggw+2n6kp47777svJJ5+c3XffPTvttFMuuOCCPPDAAxudJTvnnHPSq1evfOxjH8vHPvax/OlPf3rT47S2tuaBBx7IhRdemN69e2e33XbLKaecknvvvbdD8xgzZky+8Y1v5LHHHstJJ52UAw88MD/84Q+TJEuXLs2jjz6aSy+9NDvuuGNqampy8skn5/7772/ff9CgQfnCF76QqqqqfP7zn8+rr76apUuXdngdPvvZz2bfffdNz54989nPfjY9e/bMcccdl6qqqnzuc5/L3Llzk/ztTNKyZctyzjnnpEePHtl9993zhS98YaOzPPvvv38OPfTQVFVVZcyYMW+5XluyDkuWLMmjjz6ab33rW+nbt2+6d++e4cOHJ/nbz3Hs2LHZZ5990qNHj1xwwQV5+umn8/LLL7c/9plnnpl+/fqlV69eeeSRRzJ48OCMHTs23bp1yz777JNRo0ZlxowZSf72Gn7hhReyevXq9O3bN/vss0+HnwPAe5mLnwHeY5qbm9O3b9833X/aaafl+uuvz6mnnpokGTduXM4888x3fKxdd931Hcdra2vbvx40aFDWr1+f5cuXb8GsN7ZkyZIMHjy4/fbgwYOzYcOGtLS0tN+38847t3+9ww475LXXXnvT4yxfvjzr16/PoEGDNprnP16CtimjR4/O6NGjs379+vzyl7/M+PHjU1dXl759+2bDhg0ZMWJE+7ZtbW0brck/zzHJW87z7fzj73r16tVro8fr1atX+2MtWrQoS5YsydChQ9vHW1tbN7r9z/uuXbt2s36P6Z3W4e///bMlS5ZsFD477bRT+vXrl+bm5uy2225JNn4NLVq0KM8888ybnsffLxG99tprc+ONN+Y///M/s/fee+fCCy/MkCFDOjR/gPcyUQXwHvLMM8+kubk5+++//5vGevfuncbGxjQ2Nub555/Pl770pXziE5/IZz7zmbd9vE2dyWpqatro6+7du6d///7ZYYcd8sYbb7SPtba2ZtmyZR1+3AEDBmTRokXttxcvXpxu3bqlpqYmr7zyyjvu+4/69++f7t27Z/HixfnoRz/aPs+BAwd2+DH+rnv37jnmmGNy88035/nnn099fX169OiR3/zmN13+AQu1tbXZbbfd8tBDD2317/VW6/DXv/41K1euTJ8+fTba9p9/jq+99lpWrFix0fr/42uhtrY2w4YNy6233vqW33u//fbLjTfemPXr1+e2227LV7/61fbfUwPYlrn8D+A9YPXq1Xn44YdzwQUXZPTo0dl7773ftM3DDz+cF198MUVRpHfv3qmqqmr/6PWdd955i/4+0r333psXXnghr7/+er73ve9l1KhRqaqqyoc//OGsXbs2jzzySNavX58bb7wx69ata9+vpqYmixYtSltb21s+bn19fX784x9n4cKFWbNmTa6++uocc8wxmx0vVVVVOfroo3P11Vdn9erVWbRoUW699db2Mx+bctddd+WRRx7J6tWr09bWll/96ld54YUXst9++2XAgAE56KCD8p3vfKd9/KWXXspvf/vbDj32lq75W9lvv/3Su3fv/PCHP8wbb7yR1tbWPPfcc3nmmWfelblsah0OOeSQfOtb38pf//rXrF+/PrNmzUqSHHvssbnrrrsyd+7crFu3Lt/97nez3377tZ+l+meHHXZYFixYkGnTpmX9+vVZv359nnnmmcybNy/r1q3Lvffem1WrVqV79+7ZaaedUlVVtfmLBfAeJKoAutDfP7Ht0EMPzU033ZRTTjnlbT9O/cUXX8wpp5ySIUOGZNy4cfniF7+YAw44IMnffq/lxhtvzNChQzfrE9XGjBmTxsbGHHTQQVm3bl0uu+yyJH/7NMKJEyfmG9/4Rg455JDssMMOG11KePTRRydJDjjggHz+859/0+OOHTs2o0ePzoknnpgjjjgiPXr0yOWXX97hef2jyy+/PDvssEOOPPLINDQ0pL6+PmPHju3Qvr17985NN92Uww8/PEOHDs3UqVPzzW9+s/3ytClTpmT9+vX53Oc+l2HDhuW8887Lq6++2qHH/tKXvpQZM2Zk2LBhueKKK7bouf1dVVVVbrzxxvzpT3/KEUcckU9/+tP5xje+kdWrV3do/039/DuyDt26dcsxxxyTAw88MD/+8Y+TJJ/5zGdy/vnn59xzz82IESOycOHC9k+jfCu9e/fOLbfckgceeCAHH3xwRowYkalTp7YH+T333JORI0fmU5/6VG6//fZMmTJlc5cK4D3JH/8FAAAowZkqAACAEkQVAABACaIKAACgBFEFAABQgqgCAAAoQVQBAACU0LV/Qr4LLF++Jm1tPkUeAADYWGVlRfr332mz93vfRVVbWyGqAACAd43L/wAAAEoQVQAAACWIKgAAgBJEFQAAQAmiCgAAoARRBQAAUIKoAgAAKEFUAQAAlCCqAAAAShBVAAAAJYgqAACAEkQVAABACR2KqocffjjHHXdcxowZk2OPPTYPPfRQkmT+/PkZN25cRo0alXHjxmXBggXt+3T2GAAAQFeoKIqieKcNiqLI8OHDc9ttt2WvvfbKn/70p3zxi1/M7373u5x88skZO3ZsxowZk3vuuSd33nlnfvKTnyRJvvSlL3XqWEe1tKxOW9s7PmUAAOB9qLKyIjU1vTd/v449eGVWrVqVJFm1alUGDBiQ5cuXZ86cOamvr0+S1NfXZ86cOVm2bFlaWlo6dQwAAKCrdNvUBhUVFbnmmmty9tlnZ8cdd8yaNWvygx/8IE1NTRk4cGCqqqqSJFVVVRkwYECamppSFEWnjlVXV2+VxdmaNrQla9dv2Oz9enbvlm5+Ew4AgPeY9/P7201G1YYNG/KDH/wg3//+97P//vvnd7/7Xb72ta9lypQpnTG/7dba9Rsya27zZu83rG5guvXc5I8NAAA61fv5/e0mZz937twsWbIk+++/f5Jk//33zw477JCePXumubk5ra2tqaqqSmtra5YsWZLa2toURdGpYwAAAF1lkyfadt1117zyyiv5y1/+kiSZN29eli5dmg996EOpq6vL9OnTkyTTp09PXV1dqqurU1NT06ljAAAAXWWTn/6XJPfee29uvvnmVFRUJEnOO++8HHnkkZk3b14aGxuzcuXK9OnTJ5MnT86ee+6ZJJ0+1lHvlU//W7N2y0+P7rSNnx4FAGD7sz28v93ST//rUFRtT0QVAAC8+7aH97db9SPVAQAAeGuiCgAAoARRBQAAUIKoAgAAKEFUAQAAlCCqAAAAShBVAAAAJYgqAACAEkQVAABACaIKAACgBFEFAABQgqgCAAAoQVQBAACUIKoAAABKEFUAAAAliCoAAIASRBUAAEAJogoAAKAEUQUAAFCCqAIAAChBVAEAAJQgqgAAAEoQVQAAACWIKgAAgBJEFQAAQAmiCgAAoARRBQAAUIKoAgAAKEFUAQAAlCCqAAAAShBVAAAAJYgqAACAEkQVAABACaIKAACgBFEFAABQgqgCAAAoQVQBAACU0G1TG7z88sv5yle+0n571apVWb16dX77299m/vz5aWxszIoVK9KvX79Mnjw5e+yxR5J0+hgAAEBX2OSZqt122y333HNP+39HHHFE6uvrkyQTJ05MQ0NDZsyYkYaGhkyYMKF9v84eAwAA6AqbdfnfunXrct9992Xs2LFpaWnJnDlz2gOrvr4+c+bMybJlyzp9DAAAoKts8vK/fzRz5swMHDgw++yzT5599tkMHDgwVVVVSZKqqqoMGDAgTU1NKYqiU8eqq6vftQUBAADYHJt1purOO+/M2LFjt9ZcAAAAtjkdPlPV3NycWbNmZcqUKUmS2traNDc3p7W1NVVVVWltbc2SJUtSW1uboig6dQwAAKCrdPhM1d13351DDz00/fv3T5LU1NSkrq4u06dPT5JMnz49dXV1qa6u7vQxAACArlJRFEXRkQ1HjRqVyy67LIccckj7ffPmzUtjY2NWrlyZPn36ZPLkydlzzz27ZKyjWlpWp62tQ095q1qzdkNmzW3e7P2G1Q3MTj0361fhAABgq9se3t9WVlakpqb3Zu/X4ajaXogqAAB4920P72+3NKo264MqAAAA2JioAgAAKEFUAQAAlCCqAAAAShBVAAAAJYgqAACAEkQVAABACaIKAACgBFEFAABQgqgCAAAoQVQBAACUIKoAAABKEFUAAAAliCoAAIASRBUAAEAJogoAAKAEUQUAAFCCqAIAAChBVAEAAJQgqgAAAEoQVQAAACWIKgAAgBJEFQAAQAmiCgAAoARRBQAAUIKoAgAAKEFUAQAAlCCqAAAAShBVAAAAJYgqAACAEkQVAABACaIKAACgBFEFAABQgqgCAAAoQVQBAACUIKoAAABKEFUAAAAliCoAAIASOhRVa9euzcSJE3PUUUfl2GOPzeWXX54kmT9/fsaNG5dRo0Zl3LhxWbBgQfs+nT0GAADQFToUVVdddVV69uyZGTNm5L777sv555+fJJk4cWIaGhoyY8aMNDQ0ZMKECe37dPYYAABAV9hkVK1ZsybTpk3L+eefn4qKiiTJzjvvnJaWlsyZMyf19fVJkvr6+syZMyfLli3r9DEAAICu0m1TGyxcuDD9+vXL9ddfnyeffDI77bRTzj///PTq1SsDBw5MVVVVkqSqqioDBgxIU1NTiqLo1LHq6uqtsjgAAACbsskzVRs2bMjChQvz8Y9/PHfddVcuuuiinHvuuXnttdc6Y34AAADvaZs8UzVo0KB069at/bK7f/mXf0n//v3Tq1evNDc3p7W1NVVVVWltbc2SJUtSW1uboig6dQwAAKCrbPJMVXV1dQ444IA8/vjjSf72CXwtLS3ZY489UldXl+nTpydJpk+fnrq6ulRXV6empqZTxwAAALpKRVEUxaY2WrhwYS699NKsWLEi3bp1y1e/+tUceuihmTdvXhobG7Ny5cr06dMnkydPzp577pkknT7WUS0tq9PWtsmnvNWtWbshs+Y2b/Z+w+oGZqeemzzBCAAAnWp7eH9bWVmRmprem71fh6JqeyKqAADg3bc9vL/d0qjq0N+pAgAA4K2JKgAAgBJEFQAAQAmiCgAAoARRBQAAUIKoAgAAKEFUAQAAlCCqAAAAShBVAAAAJYgqAACAEkQVAABACaIKAACgBFEFAABQgqgCAAAoQVQBAACUIKoAAABKEFUAAAAliCoAAIASRBUAAEAJogoAAKAEUQUAAFCCqAIAAChBVAEAAJQgqgAAAEoQVQAAACWIKgAAgBJEFQAAQAmiCgAAoARRBQAAUIKoAgAAKEFUAQAAlCCqAAAAShBVAAAAJYgqAACAEkQVAABACaIKAACgBFEFAABQQoeiauTIkTn66KMzZsyYjBkzJr/+9a+TJE8//XRGjx6dUaNG5dRTT01LS0v7Pp09BgAA0BU6fKbq2muvzT333JN77rknBx98cIqiyPjx4zNhwoTMmDEjQ4cOzdSpU5Ok08cAAAC6yhZf/jd79uz07NkzQ4cOTZKccMIJefDBB7tkDAAAoKt06+iGF110UYqiyP77758LLrggTU1NGTRoUPt4dXV12trasmLFik4f69ev3xYvAAAAQBkdOlN122235d57782dd96ZoigyadKkrT0vAACAbUKHoqq2tjZJ0qNHjzQ0NOT3v/99amtrs3jx4vZtli1bloqKivTr16/TxwAAALrKJqPqtddey6pVq5L87cMiHnjggdTV1WXffffNG2+8kaeeeipJcvvtt+eYY45Jkk4fAwAA6Cqb/J2qlpaWnHvuuWltbU1bW1s+8pGPZOLEiamsrMyUKVMyceLErF27NoMHD85VV12VJJ0+BgAA0FUqiqIounoSnamlZXXa2rr+Ka9ZuyGz5jZv9n7D6gZmp54d/nwRAADoFNvD+9vKyorU1PTe/P22wlwAAADeN0QVAABACaIKAACgBFEFAABQgqgCAAAoQVQBAACUIKoAAABKEFUAAAAliCoAAIASRBUAAEAJogoAAKAEUQUAAFCCqAIAAChBVAEAAJQgqgAAAEoQVQAAACWIKgAAgBJEFQAAQAmiCgAAoARRBQAAUIKoAgAAKEFUAQAAlCCqAAAAShBVAAAAJYgqAACAEkQVAABACaIKAACgBFEFAABQgqgCAAAoQVQBAACUIKoAAABKEFUAAAAliCoAAIASRBUAAEAJogoAAKAEUQUAAFCCqAIAAChBVAEAAJSwWVF1/fXXZ++9985zzz2XJHn66aczevTojBo1KqeeempaWlrat+3sMQAAgK7Q4aj64x//mKeffjqDBg1KkhRFkfHjx2fChAmZMWNGhg4dmqlTp3bJGAAAQFfpUFStW7cukyZNysSJE1NRUZEkmT17dnr27JmhQ4cmSU444YQ8+OCDXTIGAADQVToUVd/73vcyevTo7L777u33NTU1tZ+1SpLq6uq0tbVlxYoVnT4GAADQVTYZVX/4wx8ye/bsNDQ0dMZ8AAAAtindNrXBrFmz8pe//CVHHHFEkuSVV17JaaedlpNOOimLFy9u327ZsmWpqKhIv379Ultb26ljAAAAXWWTZ6rOPPPMPPbYY5k5c2ZmzpyZXXfdNbfccktOP/30vPHGG3nqqaeSJLfffnuOOeaYJMm+++7bqWMAAABdZZNnqt5OZWVlpkyZkokTJ2bt2rUZPHhwrrrqqi4ZAwAA6CoVRVEUXT2JztTSsjptbV3/lNes3ZBZc5s3e79hdQOzU88tbmEAANgqtof3t5WVFamp6b35+22FuQAAALxviCoAAIASRBUAAEAJogoAAKAEUQUAAFCCqAIAAChBVAEAAJQgqgAAAEoQVQAAACWIKgAAgBJEFQAAQAmiCgAAoARRBQAAUIKoAgAAKEFUAQAAlCCqAAAAShBVAAAAJYgqAACAEkQVAABACaIKAACgBFEFAABQgqgCAAAoQVQBAACUIKoAAABKEFUAAAAliCoAAIASRBUAAEAJogoAAKAEUQUAAFCCqAIAAChBVAEAAJQgqgAAAEoQVQAAACWIKgAAgBJEFQAAQAmiCgAAoARRBQAAUEKHourss8/O6NGjc9xxx6WhoSFz585NksyfPz/jxo3LqFGjMm7cuCxYsKB9n84eAwAA6AodiqrJkyfn3nvvzbRp03Lqqafm0ksvTZJMnDgxDQ0NmTFjRhoaGjJhwoT2fTp7DAAAoCt0KKo+8IEPtH+9evXqVFRUpKWlJXPmzEl9fX2SpL6+PnPmzMmyZcs6fQwAAKCrdOvohpdddlkef/zxFEWRH/3oR2lqasrAgQNTVVWVJKmqqsqAAQPS1NSUoig6day6uvpdXRQAAICO6vAHVVx55ZV55JFH8rWvfS1TpkzZmnMCAADYZnT4TNXfHXfccZkwYUJ23XXXNDc3p7W1NVVVVWltbc2SJUtSW1uboig6dQwAAKCrbPJM1Zo1a9LU1NR+e+bMmenbt29qampSV1eX6dOnJ0mmT5+eurq6VFdXd/oYAABAV6koiqJ4pw2WLl2as88+O6+//noqKyvTt2/fXHLJJdlnn30yb968NDY2ZuXKlenTp08mT56cPffcM0k6fayjWlpWp63tHZ9yp1izdkNmzW3e7P2G1Q3MTj03+wQjAABsVdvD+9vKyorU1PTe7P02GVXbG1EFAADvvu3h/e2WRlWHP6gCAACANxNVAAAAJYgqAACAEkQVAABACaIKAACgBFEFAABQgqgCAAAoQVQBAACUIKoAAABKEFUAAAAliCoAAIASRBUAAEAJogoAAKAEUQUAAFCCqAIAAChBVAEAAJQgqgAAAEoQVQAAACWIKgAAgBJEFQAAQAmiCgAAoARRBQAAUIKoAgAAKEFUAQAAlCCqAAAAShBVAAAAJYgqAACAEkQVAABACaIKAACgBFEFAABQgqgCAAAoQVQBAACUIKoAAABKEFUAAAAliCoAAIASRBUAAEAJogoAAKAEUQUAAFDCJqNq+fLlOeOMMzJq1Kgce+yxOeecc7Js2bIkydNPP53Ro0dn1KhROfXUU9PS0tK+X2ePAQAAdIVNRlVFRUVOP/30zJgxI/fdd1923333TJ06NUVRZPz48ZkwYUJmzJiRoUOHZurUqUnS6WMAAABdZZNR1a9fvxxwwAHttz/5yU9m8eLFmT17dnr27JmhQ4cmSU444YQ8+OCDSdLpYwAAAF1ls36nqq2tLT/96U8zcuTINDU1ZdCgQe1j1dXVaWtry4oVKzp9DAAAoKtsVlR9+9vfzo477pgTTzxxa80HAABgm9KtoxtOnjw5L774Ym666aZUVlamtrY2ixcvbh9ftmxZKioq0q9fv04fAwAA6CodOlN19dVX59lnn80NN9yQHj16JEn23XffvPHGG3nqqaeSJLfffnuOOeaYLhkDAADoKhVFURTvtMHzzz+f+vr67LHHHunVq1eSZLfddssNN9yQ3//+95k4cWLWrl2bwYMH56qrrsrOO++cJJ0+1lEtLavT1vaOT7lTrFm7IbPmNm/2fsPqBmannh0+wQgAAJ1ie3h/W1lZkZqa3pu93yajansjqgAA4N23Pby/3dKo2qwPqgAAAGBjogoAAKAEUQUAAFCCqAIAAChBVAEAAJQgqgAAAEoQVQAAACWIKgAAgBJEFQAAQAmiCgAAoARRBQAAUIKoAgAAKEFUAQAAlCCqAAAAShBVAAAAJYgqAACAEkQVAABACaKrJQjhAAAKuUlEQVQKAACgBFEFAABQgqgCAAAoQVQBAACUIKoAAABKEFUAAAAliCoAAIASRBUAAEAJogoAAKAEUQUAAFCCqAIAAChBVAEAAJQgqgAAAEoQVQAAACWIKgAAgBJEFQAAQAmiCgAAoARRBQAAUIKoAgAAKEFUAQAAlLDJqJo8eXJGjhyZvffeO88991z7/fPnz8+4ceMyatSojBs3LgsWLOiyMQAAgK6yyag64ogjctttt2Xw4MEb3T9x4sQ0NDRkxowZaWhoyIQJE7psDAAAoKtsMqqGDh2a2traje5raWnJnDlzUl9fnySpr6/PnDlzsmzZsk4fAwAA6ErdtmSnpqamDBw4MFVVVUmSqqqqDBgwIE1NTSmKolPHqqurSy8CAADAlvJBFQAAACVs0Zmq2traNDc3p7W1NVVVVWltbc2SJUtSW1uboig6dQwAAKArbdGZqpqamtTV1WX69OlJkunTp6euri7V1dWdPgYAANCVKoqiKN5pgyuuuCIPPfRQli5dmv79+6dfv365//77M2/evDQ2NmblypXp06dPJk+enD333DNJOn1sc7S0rE5b2zs+5U6xZu2GzJrbvNn7DasbmJ16btEJRgAA2Gq2h/e3lZUVqanpvdn7bTKqtjeiCgAA3n3bw/vbLY0qH1QBAABQgqgCAAAoQVQBAACUIKoAAABKEFUAAAAliCoAAIASRBUAAEAJogoAAKAEUQUAAFCCqAIAAChBVAEAAJQgqgAAAEoQVQAAACWIKgAAgBJEFQAAQAmiCgAAoARRBQAAUIKoAgAAKEFUAQAAlCCqAAAAShBVAAAAJYgqAACAEkQVAABACaIKAACgBFEFAABQgqgCAAAoQVQBAACUIKoAAABKEFUAAAAliCoAAIASRBUAAEAJogoAAKAEUQUAAFCCqAIAAChBVAEAAJQgqgAAAEoQVQAAACWIKgAAgBK2uaiaP39+xo0bl1GjRmXcuHFZsGBBV08JAAB4H9vmomrixIlpaGjIjBkz0tDQkAkTJnT1lAAAgPexbl09gc3R0tKSOXPm5NZbb02S1NfX59vf/naWLVuW6urqDj1GZWXF1pxih3WrqsyOvbpv0X7vlecAAAB/tz28v93SeWxTUdXU1JSBAwemqqoqSVJVVZUBAwakqampw1HVv/9OW3OKm2W32r5dPQUAAHjXvF/f325zl/8BAAC8l2xTUVVbW5vm5ua0trYmSVpbW7NkyZLU1tZ28cwAAID3q20qqmpqalJXV5fp06cnSaZPn566uroOX/oHAADwbqsoiqLo6klsjnnz5qWxsTErV65Mnz59Mnny5Oy5555dPS0AAOB9apuLKgAAgPeSberyPwAAgPcaUQUAAFCCqAIAAChBVAEAAJTQrasnsD2bP39+Ghsbs2LFivTr1y+TJ0/OHnvssdE2ra2tueKKK/LrX/86FRUVOfPMM3P88cd3zYS3QR1Z4+uuuy7/8z//kwEDBiRJPvWpT2XixIldMNttz+TJkzNjxowsWrQo9913X/baa683beM1XE5H1threMstX748F198cV566aX06NEjH/rQhzJp0qQ3/SmO119/PV//+tfzxz/+MVVVVbnkkkty+OGHd9Gsty0dXePGxsY88cQT6d+/f5Lk6KOPzpe//OWumPI26eyzz87LL7+cysrK7Ljjjrn88stTV1e30TaOx+V0ZI0dj8u7/vrrc911173lv3nb9LG4YKs56aSTimnTphVFURTTpk0rTjrppDdtc/fddxennnpq0draWrS0tBQHH3xwsXDhws6e6jarI2t87bXXFt/5znc6e2rbhVmzZhWLFy8uDj/88OLPf/7zW27jNVxOR9bYa3jLLV++vPjNb37Tfvs73/lO8fWvf/1N21133XXFpZdeWhRFUcyfP7848MADi9WrV3faPLdlHV3jSy65pPjv//7vzpzadmXlypXtX//f//1fcdxxx71pG8fjcjqyxo7H5Tz77LPFaaedVhx22GFv+W/etnwsdvnfVtLS0pI5c+akvr4+SVJfX585c+Zk2bJlG233wAMP5Pjjj09lZWWqq6tz5JFH5sEHH+yKKW9zOrrGbLmhQ4emtrb2HbfxGi6nI2vMluvXr18OOOCA9tuf/OQns3jx4jdt94tf/CInnHBCkmSPPfbIvvvum0cffbTT5rkt6+gaU84HPvCB9q9Xr16dioqKN23jeFxOR9aYLbdu3bpMmjQpEydOfNu13ZaPxS7/20qampoycODAVFVVJUmqqqoyYMCANDU1bXRJRFNTUwYNGtR+u7a2Nq+88kqnz3db1NE1TpL7778/jz32WHbZZZece+65GTJkSFdMebvkNdw5vIbLa2try09/+tOMHDnyTWOLFy/O4MGD2297HW+Zd1rjJLn11lvzs5/9LLvvvnsuvPDCfOQjH+nkGW7bLrvssjz++OMpiiI/+tGP3jTueFzeptY4cTzeUt/73vcyevTo7L777m+7zbZ8LBZVbPdOOOGEnHXWWenevXsef/zxnH322XnggQfar+uH9zqv4XfHt7/97ey444458cQTu3oq2613WuOvfe1r2WWXXVJZWZlp06bl9NNPzy9/+cv2/zHGpl155ZVJkmnTpmXKlCm5+eabu3hG259NrbHj8Zb5wx/+kNmzZ+eiiy7q6qlsNS7/20pqa2vT3Nyc1tbWJH/75dElS5a86TKf2trajS6TaGpqyq677tqpc91WdXSNd9lll3Tv3j1JctBBB6W2tjbPP/98p893e+U1vPV5DZc3efLkvPjii7nmmmtSWfnmf/oGDRqURYsWtd/2Ot58m1rjgQMHtt9/3HHH5bXXXttm/g/0e81xxx2XJ598MsuXL9/ofsfjd8/brbHj8ZaZNWtW/vKXv+SII47IyJEj88orr+S0007LY489ttF22/KxWFRtJTU1Namrq8v06dOTJNOnT09dXd2bLks7+uijc8cdd6StrS3Lli3LL3/5y4waNaorprzN6egaNzc3t389d+7cLFq0KB/+8Ic7da7bM6/hrc9ruJyrr746zz77bG644Yb06NHjLbc5+uij87Of/SxJsmDBgsyePTsHH3xwZ05zm9aRNf7H1/Gvf/3rVFZWZuDAgZ01xW3amjVr0tTU1H575syZ6du3b/r167fRdo7HW66ja+x4vGXOPPPMPPbYY5k5c2ZmzpyZXXfdNbfccktGjBix0Xbb8rG4oiiKoqsnsb2aN29eGhsbs3LlyvTp0yeTJ0/OnnvumTPOOCPnnXdePvGJT6S1tTWTJk3K448/niQ544wzMm7cuC6e+bajI2t8ySWX5I9//GMqKyvTvXv3nHfeeTn00EO7eurbhCuuuCIPPfRQli5dmv79+6dfv365//77vYbfRR1ZY6/hLff888+nvr4+e+yxR3r16pUk2W233XLDDTdkzJgx+eEPf5iBAwfmtddeS2NjY+bOnZvKysqMHz8+Rx55ZBfPftvQ0TU++eST09LSkoqKivTu3TsXX3xxPvnJT3bx7LcNS5cuzdlnn53XX389lZWV6du3by655JLss88+jsfvko6usePxu2PkyJG56aabstdee203x2JRBQAAUILL/wAAAEoQVQAAACWIKgAAgBJEFQAAQAmiCgAAoARRBQAAUIKoAgAAKEFUAQAAlPD/AQVvPIAbMen+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f94d65f29b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Initial Data Visualization\n",
    "print(data.describe())\n",
    "print(len(data))\n",
    "#print(data[\"Sentiment Score\"].tolist())\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[14,7])\n",
    "sentiment_histogram = sns.distplot(data[\"Sentiment Score\"].tolist(), kde=False)#, height=7, aspect=0.9)\n",
    "sns.set()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.title(\"Distribution of Sentiment Scores\")\n",
    "plt.grid()\n",
    "#plt.savefig('figures\\sns fig1.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@octolinz16 It it counts, idk why I did either. you never talk to me anymore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>about to file taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@LettyA ahh ive always wanted to see rent  love the soundtrack!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@alydesigns i was out most of the day so didn't get much done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>@angry_barista I baked you a cake but I ated it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>this week is not going as i had hoped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>blagh class at 8 tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I hate when I have to call and wake people up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Just going to cry myself to sleep after watching Marley and Me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>im sad now  Miss.Lilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ooooh.... LOL  that leslie.... and ok I won't do it again so leslie won't  get mad again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Meh... Almost Lover is the exception... this track gets me depressed every time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599970</th>\n",
       "      <td>Thanks @eastwestchic &amp;amp; @wangyip Thanks! That was just what I was looking for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599971</th>\n",
       "      <td>@marttn thanks Martin. not the most imaginative interface, but it'll do for now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599972</th>\n",
       "      <td>@MikeJonesPhoto Congrats Mike  Way to go!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599973</th>\n",
       "      <td>http://twitpic.com/7jp4n - OMG! Office Space... I wanna steal it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599974</th>\n",
       "      <td>@yrclndstnlvr ahaha nooo you were just away from everyone else! i had to see Kara, she'd die. and yess we aree, ill see you saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599975</th>\n",
       "      <td>@BizCoachDeb  Hey, I'm baack! And, thanks so much for all those kind notes while I was gone. They made me smile at times when I needed it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599976</th>\n",
       "      <td>@mattycus Yeah, my conscience would be clear in that case.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599977</th>\n",
       "      <td>@MayorDorisWolfe Thats my girl - dishing out the &amp;quot;advice&amp;quot;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599978</th>\n",
       "      <td>@shebbs123 i second that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599979</th>\n",
       "      <td>In the garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599980</th>\n",
       "      <td>@myheartandmind jo jen by nemuselo zrovna tÃ© holce ael co nic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599981</th>\n",
       "      <td>Another Commenting Contest! [;: Yay!!!  http://tinyurl.com/m6j2an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599982</th>\n",
       "      <td>@thrillmesoon i figured out how to see my tweets and facebook status updates, and i was set  the groups seemed like a pain to set up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599983</th>\n",
       "      <td>@oxhot theri tomorrow, drinking coffee, talking about our most important and favourite issue! YOU know what I mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599984</th>\n",
       "      <td>You heard it here first -- We're having a girl. Hope it has my looks and Wendy's brains. (Kidding, babe).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599985</th>\n",
       "      <td>if ur the lead singer in a band, beware falling prey to LSD &amp;quot;Lead Singer Disease&amp;quot; http://tinyurl.com/n65xjt  #music #haveyouever?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599986</th>\n",
       "      <td>@tarayqueen too much ads on my blog.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599987</th>\n",
       "      <td>@La_r_a NEVEER  I think that you both will get on well with each other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599988</th>\n",
       "      <td>@Roy_Everitt ha- good job. that's right - we gotta throw that #bigrun tag EVERYWHERE! I wanna get it trending before I start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599989</th>\n",
       "      <td>@Ms_Hip_Hop im glad ur doing well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599990</th>\n",
       "      <td>WOOOOO! Xbox is back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599991</th>\n",
       "      <td>@rmedina @LaTati Mmmm  That sounds absolutely perfect... but my schedule is full. I won't have time to lay in bed until Sunday. Ugh!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599992</th>\n",
       "      <td>ReCoVeRiNg FrOm ThE lOnG wEeKeNd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599993</th>\n",
       "      <td>@SCOOBY_GRITBOYS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>@Cliff_Forster Yeah, that does work better than just waiting for it  In the end I just wonder if I have time to keep up a good blog.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>Just woke up. Having no school is the best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                               Tweet\n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D                        \n",
       "1        is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!                            \n",
       "2        @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                                                  \n",
       "3        my whole body feels itchy and like its on fire                                                                                             \n",
       "4        @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.                             \n",
       "5        @Kwesidei not the whole crew                                                                                                               \n",
       "6        Need a hug                                                                                                                                 \n",
       "7        @LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                                        \n",
       "8        @Tatiana_K nope they didn't have it                                                                                                        \n",
       "9        @twittera que me muera ?                                                                                                                   \n",
       "10       spring break in plain city... it's snowing                                                                                                 \n",
       "11       I just re-pierced my ears                                                                                                                  \n",
       "12       @caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                                             \n",
       "13       @octolinz16 It it counts, idk why I did either. you never talk to me anymore                                                               \n",
       "14       @smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.                      \n",
       "15       @iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!                                    \n",
       "16       Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                                              \n",
       "17       about to file taxes                                                                                                                        \n",
       "18       @LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                                           \n",
       "19       @FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                                             \n",
       "20       @alydesigns i was out most of the day so didn't get much done                                                                              \n",
       "21       one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh*                                       \n",
       "22       @angry_barista I baked you a cake but I ated it                                                                                            \n",
       "23       this week is not going as i had hoped                                                                                                      \n",
       "24       blagh class at 8 tomorrow                                                                                                                  \n",
       "25       I hate when I have to call and wake people up                                                                                              \n",
       "26       Just going to cry myself to sleep after watching Marley and Me.                                                                            \n",
       "27       im sad now  Miss.Lilly                                                                                                                     \n",
       "28       ooooh.... LOL  that leslie.... and ok I won't do it again so leslie won't  get mad again                                                   \n",
       "29       Meh... Almost Lover is the exception... this track gets me depressed every time.                                                           \n",
       "...                                                                                    ...                                                          \n",
       "1599970  Thanks @eastwestchic &amp; @wangyip Thanks! That was just what I was looking for                                                           \n",
       "1599971  @marttn thanks Martin. not the most imaginative interface, but it'll do for now                                                            \n",
       "1599972  @MikeJonesPhoto Congrats Mike  Way to go!                                                                                                  \n",
       "1599973  http://twitpic.com/7jp4n - OMG! Office Space... I wanna steal it.                                                                          \n",
       "1599974  @yrclndstnlvr ahaha nooo you were just away from everyone else! i had to see Kara, she'd die. and yess we aree, ill see you saturday       \n",
       "1599975  @BizCoachDeb  Hey, I'm baack! And, thanks so much for all those kind notes while I was gone. They made me smile at times when I needed it! \n",
       "1599976  @mattycus Yeah, my conscience would be clear in that case.                                                                                 \n",
       "1599977  @MayorDorisWolfe Thats my girl - dishing out the &quot;advice&quot;                                                                        \n",
       "1599978  @shebbs123 i second that                                                                                                                   \n",
       "1599979  In the garden                                                                                                                              \n",
       "1599980  @myheartandmind jo jen by nemuselo zrovna tÃ© holce ael co nic                                                                             \n",
       "1599981  Another Commenting Contest! [;: Yay!!!  http://tinyurl.com/m6j2an                                                                          \n",
       "1599982  @thrillmesoon i figured out how to see my tweets and facebook status updates, and i was set  the groups seemed like a pain to set up...    \n",
       "1599983  @oxhot theri tomorrow, drinking coffee, talking about our most important and favourite issue! YOU know what I mean                         \n",
       "1599984  You heard it here first -- We're having a girl. Hope it has my looks and Wendy's brains. (Kidding, babe).                                  \n",
       "1599985  if ur the lead singer in a band, beware falling prey to LSD &quot;Lead Singer Disease&quot; http://tinyurl.com/n65xjt  #music #haveyouever?\n",
       "1599986  @tarayqueen too much ads on my blog.                                                                                                       \n",
       "1599987  @La_r_a NEVEER  I think that you both will get on well with each other...                                                                  \n",
       "1599988  @Roy_Everitt ha- good job. that's right - we gotta throw that #bigrun tag EVERYWHERE! I wanna get it trending before I start               \n",
       "1599989  @Ms_Hip_Hop im glad ur doing well                                                                                                          \n",
       "1599990  WOOOOO! Xbox is back                                                                                                                       \n",
       "1599991  @rmedina @LaTati Mmmm  That sounds absolutely perfect... but my schedule is full. I won't have time to lay in bed until Sunday. Ugh!!      \n",
       "1599992  ReCoVeRiNg FrOm ThE lOnG wEeKeNd                                                                                                           \n",
       "1599993  @SCOOBY_GRITBOYS                                                                                                                           \n",
       "1599994  @Cliff_Forster Yeah, that does work better than just waiting for it  In the end I just wonder if I have time to keep up a good blog.       \n",
       "1599995  Just woke up. Having no school is the best feeling ever                                                                                    \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta                                                             \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me for details                                                                                   \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur                                                                           \n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H                                                                              \n",
       "\n",
       "[1600000 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Positive']\n",
      "1600000\n"
     ]
    }
   ],
   "source": [
    "## Manually convert sentiment scores to positive/negative binary categories and move to target array\n",
    "\n",
    "X = data.copy()\n",
    "y = [\"Positive\" if sentiment == 4 else \"Negative\" for sentiment in X[\"Sentiment Score\"]]\n",
    "\n",
    "#Since sentiment can be considered to be ordinal, use label encoder\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "\n",
    "X = X.drop(\"Sentiment Score\", axis=1)\n",
    "display(X)\n",
    "display(y)\n",
    "print(le.classes_)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644091</th>\n",
       "      <td>God I'm sort of annoying its like great+ awful+fabulous at the same time!  hahahahah shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721723</th>\n",
       "      <td>In VT, missing my hubby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057340</th>\n",
       "      <td>just got a present from apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628980</th>\n",
       "      <td>@martinsays I want the new album â¥kinda of impossible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172036</th>\n",
       "      <td>@LizzieGrubman have a great time in the Hamptons. Hope it's for some relaxation, not for work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298089</th>\n",
       "      <td>@bunnydozer Not lazy, missy. Actually been crazy busy and they're only open during certain hours. Plus they're 30min away from me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391122</th>\n",
       "      <td>Hangovers are no fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499323</th>\n",
       "      <td>@jennettemccurdy watching a sad episode of One Tree Hill  on the other hand I hear you've been nominated for Best TV Sidekick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70209</th>\n",
       "      <td>@kinagrannis kina! http://tinyurl.com/ct546v why no Uke??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329322</th>\n",
       "      <td>@shani_texas its almost over girl! I'm feeling giddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217173</th>\n",
       "      <td>&amp;quot;He was taking it for a long time, then it started getting to him. It was getting to Dale Jr.&amp;quot; http://snipr.com/j3rsn - Tony Sr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807476</th>\n",
       "      <td>i have just been woken up grrr was having a nice dream too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392455</th>\n",
       "      <td>@a_liss_a I rate you my most interesting person I follow!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718830</th>\n",
       "      <td>well, i have to go to work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173319</th>\n",
       "      <td>@sugabear906 Morning hun! How are you? Love the necklace, it's so cute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124004</th>\n",
       "      <td>maths exam at 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394932</th>\n",
       "      <td>i am so absolutely exhausted. that one hour of sleep did me no justice. oh well. time for breakfast, then the creation museum!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121473</th>\n",
       "      <td>@RhapsodyInBleh Misses you! I wish you could feed me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440725</th>\n",
       "      <td>leaving the beach tomorrow morning. i'm glad i'm going home!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422393</th>\n",
       "      <td>@syifachipuy wahh sayang bgt yaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199447</th>\n",
       "      <td>Yay for being in the showroom all day and not getting to have fun last night  I'm so upset I just used an emoticon. Yay three jobs!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506483</th>\n",
       "      <td>Im HOME!!! and missing the mountains already...  but civilization is very nice too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386767</th>\n",
       "      <td>On the plane flying higher to jump!  http://short.to/dlnc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287577</th>\n",
       "      <td>@HettiSpaghetti Dont make me jealous, it was so hot in the office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194141</th>\n",
       "      <td>regrets losing touch with her highschool best friends. Timothy and Crisselle, I miss you  http://plurk.com/p/x63yf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467338</th>\n",
       "      <td>@lissyvz oh ok!! that's what i was askin...already took pill. wondered how long til i could have breakfast...1/2 hr. is much better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128330</th>\n",
       "      <td>Jackie Brambles is increasingly becoming my favourite loose woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345774</th>\n",
       "      <td>@Annjj fuck me he will love it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425258</th>\n",
       "      <td>I want to talk to jessica or ashlee simpson  please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444301</th>\n",
       "      <td>While I'm at it, I also want a Fluminense shirt.  I think a weekend away to get one is out of the question though.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470485</th>\n",
       "      <td>@BobMaher Having read War and Peace, I would be inclined to argue that it was written by 199 yammering monkeys.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396025</th>\n",
       "      <td>Thank you everyone for the follwing!!! Means so much to us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184779</th>\n",
       "      <td>@RobinTaylorRoth 2,000,000 sq. ft building now totally empty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262752</th>\n",
       "      <td>http://bit.ly/h7Gtc  The real meaning of twitter, a must read  WE are one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284372</th>\n",
       "      <td>Good morning, (it is for ten more minutes anyway!) Exercised;ellipitical trainer +weights,feeling slightly more cheerful, sun is shining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103355</th>\n",
       "      <td>@chrisluvssixxam miss ya already...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791743</th>\n",
       "      <td>is having a bad day already.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247617</th>\n",
       "      <td>@BeaWise can't wait to hear!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327069</th>\n",
       "      <td>Cramped up towards the end of my run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370455</th>\n",
       "      <td>Saw her graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787201</th>\n",
       "      <td>ahh. bimbo..what the - is she trying to do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113396</th>\n",
       "      <td>Finally decided with my teacher that the Masterclass will take place next school year. Sorry to my classmates! Going to cook salmon now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329365</th>\n",
       "      <td>Frikin nervous for science exam tomorrow, it has 175 QUESTIONS!!!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41090</th>\n",
       "      <td>im in work AGAIN cant wait to go home , still feel funny from my migraine i had last night !!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278167</th>\n",
       "      <td>ok back to work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239911</th>\n",
       "      <td>learned some actionscript (3) over the weekend. might start doing more of that flash stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175203</th>\n",
       "      <td>Greg Pritchard was robbed  ii am too gutted for words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912756</th>\n",
       "      <td>Btw; happy mothers day! This week its all about my momma; mothersday dinnner-birthday dinner-surprise bday party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136074</th>\n",
       "      <td>I think it's time to go to bed  Night Night Everyone ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570006</th>\n",
       "      <td>@PreternaReviews yeah, but, dude, that's the key here. I'm a CHICK. Chicks wear pink. Especially the ultra cool rocker chicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999890</th>\n",
       "      <td>TF2 has updated but its way to late to play.  looks like I'll be spying and sniping my heart out tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137337</th>\n",
       "      <td>Last free travel at AP-1  http://yfrog.com/0uvu5j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103462</th>\n",
       "      <td>@cloecouturier Yes, I do have a few ..   4 Girls ...   You son is 23 ... all grown up now .. happens fast!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732180</th>\n",
       "      <td>Thunderstorms yesterday, more on the way. Looks like I won't be online much again today.  HAPPY FATHER'S DAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110268</th>\n",
       "      <td>im starting my many hours of work now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>this song's middle change just doesn't want to be born..... arghhhh!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414414</th>\n",
       "      <td>@officialnjonas Good luck with that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>@ProudGamerTweet I rather average 32370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671155</th>\n",
       "      <td>Pickin up @misstinayao waitin on @sadittysash 2 hurry up...I odeeee missed dem  Table talk 2nite...LOL bout to be fat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>@ home studying for maths wooot ! im so going to fail this shit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                               Tweet\n",
       "644091   God I'm sort of annoying its like great+ awful+fabulous at the same time!  hahahahah shit                                                  \n",
       "721723   In VT, missing my hubby                                                                                                                    \n",
       "1057340  just got a present from apple                                                                                                              \n",
       "628980   @martinsays I want the new album â¥kinda of impossible                                                                                    \n",
       "172036   @LizzieGrubman have a great time in the Hamptons. Hope it's for some relaxation, not for work                                              \n",
       "298089   @bunnydozer Not lazy, missy. Actually been crazy busy and they're only open during certain hours. Plus they're 30min away from me          \n",
       "391122   Hangovers are no fun                                                                                                                       \n",
       "499323   @jennettemccurdy watching a sad episode of One Tree Hill  on the other hand I hear you've been nominated for Best TV Sidekick              \n",
       "70209    @kinagrannis kina! http://tinyurl.com/ct546v why no Uke??                                                                                  \n",
       "1329322  @shani_texas its almost over girl! I'm feeling giddy                                                                                       \n",
       "217173   &quot;He was taking it for a long time, then it started getting to him. It was getting to Dale Jr.&quot; http://snipr.com/j3rsn - Tony Sr. \n",
       "807476   i have just been woken up grrr was having a nice dream too                                                                                 \n",
       "1392455  @a_liss_a I rate you my most interesting person I follow!!                                                                                 \n",
       "718830   well, i have to go to work                                                                                                                 \n",
       "1173319  @sugabear906 Morning hun! How are you? Love the necklace, it's so cute                                                                     \n",
       "124004   maths exam at 1                                                                                                                            \n",
       "1394932  i am so absolutely exhausted. that one hour of sleep did me no justice. oh well. time for breakfast, then the creation museum!             \n",
       "121473   @RhapsodyInBleh Misses you! I wish you could feed me                                                                                       \n",
       "1440725  leaving the beach tomorrow morning. i'm glad i'm going home!                                                                               \n",
       "422393   @syifachipuy wahh sayang bgt yaa                                                                                                           \n",
       "199447   Yay for being in the showroom all day and not getting to have fun last night  I'm so upset I just used an emoticon. Yay three jobs!        \n",
       "1506483  Im HOME!!! and missing the mountains already...  but civilization is very nice too.                                                        \n",
       "1386767  On the plane flying higher to jump!  http://short.to/dlnc                                                                                  \n",
       "287577   @HettiSpaghetti Dont make me jealous, it was so hot in the office                                                                          \n",
       "194141   regrets losing touch with her highschool best friends. Timothy and Crisselle, I miss you  http://plurk.com/p/x63yf                         \n",
       "1467338  @lissyvz oh ok!! that's what i was askin...already took pill. wondered how long til i could have breakfast...1/2 hr. is much better        \n",
       "128330   Jackie Brambles is increasingly becoming my favourite loose woman                                                                          \n",
       "1345774  @Annjj fuck me he will love it                                                                                                             \n",
       "1425258  I want to talk to jessica or ashlee simpson  please                                                                                        \n",
       "444301   While I'm at it, I also want a Fluminense shirt.  I think a weekend away to get one is out of the question though.                         \n",
       "...                                                                                                                       ...                       \n",
       "1470485  @BobMaher Having read War and Peace, I would be inclined to argue that it was written by 199 yammering monkeys.                            \n",
       "1396025  Thank you everyone for the follwing!!! Means so much to us                                                                                 \n",
       "184779   @RobinTaylorRoth 2,000,000 sq. ft building now totally empty.                                                                              \n",
       "1262752  http://bit.ly/h7Gtc  The real meaning of twitter, a must read  WE are one.                                                                 \n",
       "1284372  Good morning, (it is for ten more minutes anyway!) Exercised;ellipitical trainer +weights,feeling slightly more cheerful, sun is shining   \n",
       "103355   @chrisluvssixxam miss ya already...                                                                                                        \n",
       "791743   is having a bad day already.                                                                                                               \n",
       "1247617  @BeaWise can't wait to hear!                                                                                                               \n",
       "327069   Cramped up towards the end of my run                                                                                                       \n",
       "1370455  Saw her graduate                                                                                                                           \n",
       "787201   ahh. bimbo..what the - is she trying to do                                                                                                 \n",
       "1113396  Finally decided with my teacher that the Masterclass will take place next school year. Sorry to my classmates! Going to cook salmon now    \n",
       "329365   Frikin nervous for science exam tomorrow, it has 175 QUESTIONS!!!!!!!!!!                                                                   \n",
       "41090    im in work AGAIN cant wait to go home , still feel funny from my migraine i had last night !!                                              \n",
       "278167   ok back to work                                                                                                                            \n",
       "1239911  learned some actionscript (3) over the weekend. might start doing more of that flash stuff                                                 \n",
       "175203   Greg Pritchard was robbed  ii am too gutted for words                                                                                      \n",
       "912756   Btw; happy mothers day! This week its all about my momma; mothersday dinnner-birthday dinner-surprise bday party                           \n",
       "1136074  I think it's time to go to bed  Night Night Everyone ?                                                                                     \n",
       "1570006  @PreternaReviews yeah, but, dude, that's the key here. I'm a CHICK. Chicks wear pink. Especially the ultra cool rocker chicks              \n",
       "999890   TF2 has updated but its way to late to play.  looks like I'll be spying and sniping my heart out tomorrow                                  \n",
       "137337   Last free travel at AP-1  http://yfrog.com/0uvu5j                                                                                          \n",
       "1103462  @cloecouturier Yes, I do have a few ..   4 Girls ...   You son is 23 ... all grown up now .. happens fast!!                                \n",
       "732180   Thunderstorms yesterday, more on the way. Looks like I won't be online much again today.  HAPPY FATHER'S DAY                               \n",
       "110268   im starting my many hours of work now                                                                                                      \n",
       "259178   this song's middle change just doesn't want to be born..... arghhhh!!                                                                      \n",
       "1414414  @officialnjonas Good luck with that                                                                                                        \n",
       "131932   @ProudGamerTweet I rather average 32370                                                                                                    \n",
       "671155   Pickin up @misstinayao waitin on @sadittysash 2 hurry up...I odeeee missed dem  Table talk 2nite...LOL bout to be fat...                   \n",
       "121958   @ home studying for maths wooot ! im so going to fail this shit                                                                            \n",
       "\n",
       "[160000 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "160000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Split data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "#display(X_train)\n",
    "\n",
    "display(X_train)\n",
    "#display(X_valid)\n",
    "display(y_train)\n",
    "display(len(y_train))\n",
    "#display(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_valid_2, y_train_2, y_valid_2 = train_test_split(X, y, test_size=0.9, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"anova_filter = SelectKBest(f_regression, k=5)\\nclf = svm.SVC(kernel='linear')\\nanova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])\\n# You can set the parameters using the names issued\\n# For instance, fit using a k of 10 in the SelectKBest\\n# and a parameter 'C' of the svm\\nanova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)\\n                      \\nPipeline(memory=None,\\n         steps=[('anova', SelectKBest(...)),\\n                ('svc', SVC(...))])\\nprediction = anova_svm.predict(X)\\n#anova_svm.score(X, y)                        \\n#0.83\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example \n",
    "# ANOVA SVM-C\n",
    "'''anova_filter = SelectKBest(f_regression, k=5)\n",
    "clf = svm.SVC(kernel='linear')\n",
    "anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])\n",
    "# You can set the parameters using the names issued\n",
    "# For instance, fit using a k of 10 in the SelectKBest\n",
    "# and a parameter 'C' of the svm\n",
    "anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)\n",
    "                      \n",
    "Pipeline(memory=None,\n",
    "         steps=[('anova', SelectKBest(...)),\n",
    "                ('svc', SVC(...))])\n",
    "prediction = anova_svm.predict(X)\n",
    "#anova_svm.score(X, y)                        \n",
    "#0.83'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create preprocessing pipelines\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "## Column selector (removes non-essential columns as defined by user), X must be a pandas dataframe\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "        try:\n",
    "            return X[self.columns]\n",
    "        except KeyError:\n",
    "            cols_error = list(set(self.columns) - set(X.columns))\n",
    "            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom transforms and preprocessing pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numeric features if applicable\n",
    "#numeric_features = ['age', 'fare']\n",
    "#numeric_transformer = Pipeline(steps=[\n",
    "#    ('imputer', SimpleImputer(strategy='median')),\n",
    "#    ('scaler', StandardScaler())])\n",
    "\n",
    "## Categorical features if applicable\n",
    "#categorical_features = [\"Sentiment Score\"]\n",
    "#categorical_transformer = Pipeline(steps=[\n",
    "#    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "#preprocessor = ColumnTransformer(\n",
    "#    transformers=[\n",
    "#        ('num', numeric_transformer, numeric_features),\n",
    "#        (\"cat\", categorical_transformer, categorical_features)])\n",
    "\n",
    "## Create custom transformer\n",
    "class tweet_cleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if(self.verbose):\n",
    "            print(\"Verbose mode on!\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        #display(X)\n",
    "        #Use beautifulsoup to decode HTML code\n",
    "        print(\"Preprocessing...\")\n",
    "        X_1 = [(BeautifulSoup(tweet,\"lxml\").get_text()) for tweet in X[\"Tweet\"]]\n",
    "        #X_2 = [re.sub(r\"@[A-Za-z0-9]+\",\"\",tweet) for tweet in X_1]\n",
    "        X_3 = [re.sub(\"https?://[A-Za-z0-9./]+\",\"\",tweet) for tweet in X_1]\n",
    "        X_4 = [re.sub(\"www.[A-Za-z0-9./]+\",\"\",tweet) for tweet in X_3]\n",
    "        #X_5 = [(tweet.decode(\"utf-8-sig\")).replace(u\"\\ufffd\",\"\") for tweet in X_4]\n",
    "        #X_5 = [re.sub(\"[^a-zA-Z]\",\" \", tweet) for tweet in X_4]\n",
    "        #display(X_4)\n",
    "        print(\"Preprocess Complete\")\n",
    "\n",
    "        return X_4\n",
    "    \n",
    "\n",
    "class keras_tokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if(self.verbose):\n",
    "            print(\"Verbose mode on!\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        #display(X)\n",
    "        ## Use Keras tokenizer and padding for neural net inputs\n",
    "        print(\"Tokenizing...\")\n",
    "        tokenizer = Tokenizer(nb_words=2500, lower=True,split=' ')\n",
    "        tokenizer.fit_on_texts(X[\"Tweet\"])\n",
    "        #print(tokenizer.word_index)  # To see the dicstionary\n",
    "        X = tokenizer.texts_to_sequences(X[\"Tweet\"])\n",
    "        X = pad_sequences(X)\n",
    "        display(X)\n",
    "        print(\"Tokenization Complete\")\n",
    "\n",
    "        return X\n",
    "\n",
    "## Categorical features if applicable\n",
    "tweet_col = [\"Tweet\"]\n",
    "tweet_transformer = Pipeline(steps=[\n",
    "    (\"cleaner\", tweet_cleaner()),\n",
    "    (\"tfidf_vectorizer\", TfidfVectorizer())\n",
    "    ])\n",
    "\n",
    "tweet_transformer_keras = Pipeline(steps=[\n",
    "    (\"cleaner\", tweet_cleaner()),\n",
    "    (\"keras_tokenizer\", keras_tokenizer())\n",
    "    ])\n",
    "                      \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tweet_transformer\", tweet_transformer, tweet_col)\n",
    "        ])\n",
    "\n",
    "preprocessor_keras = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tweet_transformer_keras\", tweet_transformer_keras, tweet_col)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w...obs=None,\n",
      "            oob_score=False, random_state=None, verbose=2,\n",
      "            warm_start=False))])\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w...    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=2, warm_start=False))])\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w..., reg_lambda=0.0, silent=False,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0))])\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w...lpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1, verbosity=1))])\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w...='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=2))])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'KerasClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9869272e383e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# wrap the model using the function you created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mLSTM_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_LSTM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m LSTM_clf_fullpipe = Pipeline(steps=[\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KerasClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "## Initialize classifiers\n",
    "\n",
    "## Random Forest\n",
    "rf_clf = RandomForestClassifier(verbose=2)\n",
    "rf_clf_fullpipe = Pipeline(steps=[\n",
    "                            (\"preprocessor\", preprocessor),\n",
    "                            (\"rf_classifier\", rf_clf)\n",
    "                            ])\n",
    "print(rf_clf_fullpipe)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "## Gradient Boost\n",
    "gb_clf = GradientBoostingClassifier(verbose=2)\n",
    "gb_clf_fullpipe = Pipeline(steps=[\n",
    "                            (\"preprocessor\", preprocessor),\n",
    "                            (\"gb_classifier\", gb_clf)\n",
    "                            ])\n",
    "print(gb_clf_fullpipe)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "## LightGBM\n",
    "lgbm_clf = LGBMClassifier(silent=False,n_jobs=1)\n",
    "lgbm_clf_fullpipe = Pipeline(steps=[\n",
    "                                (\"preprocessor\", preprocessor),\n",
    "                                (\"lgbm_classifier\", lgbm_clf)\n",
    "                            ])\n",
    "print(lgbm_clf_fullpipe)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "## XGBoost\n",
    "xgb_clf = XGBClassifier(verbosity=1)\n",
    "xgb_clf_fullpipe = Pipeline(steps=[\n",
    "                                (\"preprocessor\", preprocessor),\n",
    "                                (\"xgb_classifier\", xgb_clf)\n",
    "                            ])\n",
    "print(xgb_clf_fullpipe)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "## Support Vector Classification\n",
    "svm_clf = SVC(verbose=2)\n",
    "svm_clf_fullpipe = Pipeline(steps=[\n",
    "                                (\"preprocessor\", preprocessor),\n",
    "                                (\"svm_classifier\", svm_clf)\n",
    "                            ])\n",
    "print(svm_clf_fullpipe)\n",
    "\n",
    "## Long short-term memory neural network\n",
    "def create_LSTM(embed_dim,lstm_out,batch_size,dropout,dropout_U,dropout_W):\n",
    "    #embed_dim = 128\n",
    "    #lstm_out = 200\n",
    "    #batch_size = 32\n",
    "    #dropout = 0.2\n",
    "    #dropout_U = 0.2\n",
    "    #dropout_W = 0.2\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(2500,embed_dim,input_length=X_train.shape[1],dropout=dropout))\n",
    "    model.add(layers.LSTM(lstm_out,dropout_U=dropout_U,dropout_W=dropout_W))\n",
    "    model.add(layers.Dense(2,activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "# wrap the model using the function you created\n",
    "LSTM_clf = KerasClassifier(build_fn=create_LSTM,verbose=2)\n",
    "\n",
    "LSTM_clf_fullpipe = Pipeline(steps=[\n",
    "                                (\"preprocessor_keras\", preprocessor_keras),\n",
    "                                (\"LSTM_classifier\", LSTM_clf)\n",
    "                            ])\n",
    "print(LSTM_clf_fullpipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiate a full preprocessing and prediction pipeline\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(10,110,11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "gamma = [x for x in np.logspace(-15,3,base=2,num=7)]\n",
    "gamma.append(\"scale\")\n",
    "gamma.append(\"auto\")\n",
    "\n",
    "## RandomizedSearchCV hyperparameter optimization\n",
    "rf_param_grid = {\n",
    "    \"rf_classifier__n_estimators\": [int(x) for x in np.linspace(50,500,11)],\n",
    "    \"rf_classifier__max_depth\": max_depth,\n",
    "    \"rf_classifier__max_features\": [\"auto\"],\n",
    "    \"rf_classifier__min_samples_split\": [2, 10, 100],\n",
    "    \"rf_classifier__min_samples_leaf\": [1, 2, 4, 10],\n",
    "    }\n",
    "\n",
    "gb_param_grid = {\n",
    "    \"gb_classifier__n_estimators\": [int(x) for x in np.linspace(50,500,11)],\n",
    "    \"gb_classifier__max_depth\": max_depth\n",
    "    }\n",
    "\n",
    "lgbm_param_grid = {\n",
    "    \"lgbm_classifier__n_estimators\": [int(x) for x in np.linspace(50,550,11)],\n",
    "    \"lgbm_classifier__max_depth\": max_depth,\n",
    "    }\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"xgb_classifier__n_estimators\": [int(x) for x in np.linspace(50,550,11)],\n",
    "    \"xgb_classifier__max_depth\": max_depth,  \n",
    "    }\n",
    "\n",
    "svm_param_grid = {\n",
    "    \"svm_classifier__C\": [x for x in np.logspace(-5,15,base=2,num=7)],\n",
    "    \"svm_classifier__gamma\": gamma\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    #\"preprocessor__tweet_transformer__tfidf_vectorizer__max_df\": np.linspace(0.2, 1, 10),\n",
    "    \"preprocessor__tweet_transformer__tfidf_vectorizer__binary\": [True],\n",
    "    \"preprocessor__tweet_transformer__tfidf_vectorizer__token_pattern\": [r\"(?u)\\b\\w\\w+\\b|\\'\"],\n",
    "    \"preprocessor__tweet_transformer__tfidf_vectorizer__ngram_range\": [(1,2)],\n",
    "    }\n",
    "\n",
    "rf_param_grid.update(param_grid)\n",
    "gb_param_grid.update(param_grid)\n",
    "lgbm_param_grid.update(param_grid)\n",
    "xgb_param_grid.update(param_grid)\n",
    "svm_param_grid.update(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of pipelines and classifier types for ease of reference\n",
    "pipe_dict = {\n",
    "            \"RandomForest\" : (rf_clf_fullpipe, rf_param_grid),\n",
    "            \"GradientBoosting\" : (gb_clf_fullpipe, gb_param_grid),\n",
    "            \"XGBoost\" : (xgb_clf_fullpipe, xgb_param_grid),\n",
    "            \"LightGBM\" : (lgbm_clf_fullpipe, lgbm_param_grid),\n",
    "            \"SVC\" : (svm_clf_fullpipe, svm_param_grid),\n",
    "            \"VotingClassifier\" : (vote_clf_fullpipe,)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to fit a model pipeline\n",
    "def pipeline_fit(pipeline, cv, iid, verbose, n_jobs, optimization):\n",
    "\n",
    "    if optimization == True:\n",
    "        ## Do hyperparameter optimization using RandomizedSearchCV\n",
    "        \n",
    "        rand_search = RandomizedSearchCV(pipe_dict[pipeline][0], pipe_dict[pipeline][1], cv=cv, iid=iid, verbose=verbose, n_jobs=n_jobs)\n",
    "\n",
    "        ## Train the model\n",
    "        print(\"Training \"+pipeline+\" on \"+str(len(y_train))+\" samples...\")\n",
    "        rand_search.fit(X_train, y_train)\n",
    "\n",
    "        print(\"\")\n",
    "        print(pipeline+\":\")\n",
    "        print(\"Best Score: \", rand_search.best_score_)\n",
    "        print(\"Best Params: \", rand_search.best_params_)\n",
    "\n",
    "        ## Use joblib to dump the best estimator to joblib file for persistence purposes\n",
    "        clf_final = rand_search.best_estimator_\n",
    "        dump(clf_final, \"twitter_\"+pipeline+\".joblib\")\n",
    "\n",
    "        return clf_final\n",
    "    \n",
    "    else:\n",
    "        ## No optimization is performed, fit to model only\n",
    "        print(\"Training \"+pipeline+\" on \"+str(len(y_train))+\" samples...\")\n",
    "        print(pipe_dict[pipeline][0].steps[1])\n",
    "        clf_final = pipe_dict[pipeline][0].fit(X_train, y_train)\n",
    "\n",
    "        ## Use joblib to dump the best estimator to joblib file for persistence purposes\n",
    "        dump(clf_final, \"twitter_\"+pipeline+\".joblib\")\n",
    "\n",
    "        return clf_final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM on 160000 samples...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 296.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM:\n",
      "Best Score:  0.79089375\n",
      "Best Params:  {'preprocessor__tweet_transformer__tfidf_vectorizer__token_pattern': \"(?u)\\\\b\\\\w\\\\w+\\\\b|\\\\'\", 'preprocessor__tweet_transformer__tfidf_vectorizer__ngram_range': (1, 2), 'preprocessor__tweet_transformer__tfidf_vectorizer__binary': True, 'lgbm_classifier__n_estimators': 500, 'lgbm_classifier__max_depth': 110}\n"
     ]
    }
   ],
   "source": [
    "## Activate full pipeline\n",
    "lgbm_clf_final = pipeline_fit(pipeline=\"LightGBM\", cv=5, iid=False, verbose=1, n_jobs=1, optimization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost on 160000 samples...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-600977bfb006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxgb_clf_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"XGBoost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-99-3a81b819263a>\u001b[0m in \u001b[0;36mpipeline_fit\u001b[0;34m(pipeline, cv, iid, verbose, n_jobs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m## Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" on \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" samples...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mrand_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1514\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    711\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1110\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgb_clf_final = pipeline_fit(pipeline=\"XGBoost\", cv=5, iid=False, verbose=1, n_jobs=1, optimization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForest on 160000 samples...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    3.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:   24.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:   24.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:   23.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:   24.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:   25.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   54.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   13.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   51.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   14.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   51.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   14.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   49.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   13.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   50.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   14.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   25.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   25.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    6.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   25.1s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   25.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    5.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   26.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:  3.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:    7.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:  3.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:  3.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:    7.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:  3.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:    8.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:  3.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:    8.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   10.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   10.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    9.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   10.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:   10.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 185 out of 185 | elapsed:    9.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:    8.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:   36.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:   37.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:    8.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:   35.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:   29.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:    6.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 455 out of 455 | elapsed:   29.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 17.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   21.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 17.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   22.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 17.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   21.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 17.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   22.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 18.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   21.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   49.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    6.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   26.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   48.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   24.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   59.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   10.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   41.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   28.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   56.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   25.6s finished\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 288.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 36.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForest:\n",
      "Best Score:  0.7796625\n",
      "Best Params:  {'rf_classifier__n_estimators': 500, 'rf_classifier__min_samples_split': 100, 'rf_classifier__min_samples_leaf': 2, 'rf_classifier__max_features': 'auto', 'rf_classifier__max_depth': None, 'preprocessor__tweet_transformer__tfidf_vectorizer__token_pattern': \"(?u)\\\\b\\\\w\\\\w+\\\\b|\\\\'\", 'preprocessor__tweet_transformer__tfidf_vectorizer__ngram_range': (1, 2), 'preprocessor__tweet_transformer__tfidf_vectorizer__binary': True}\n"
     ]
    }
   ],
   "source": [
    "rf_clf_final = pipeline_fit(pipeline=\"RandomForest\", cv=5, iid=False, verbose=1, n_jobs=1, optimization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_final = pipeline_fit(pipeline=\"SVC\", cv=5, iid=False, verbose=2, n_jobs=1, optimization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load estimator via joblib\n",
    "\n",
    "lgbm_clf_final = load(\"twitter_LightGBM.joblib\")\n",
    "rf_clf_final = load(\"twitter_RandomForest.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.1, max_depth=110,\n",
      "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "        n_estimators=500, n_jobs=1, num_leaves=31, objective=None,\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=100,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=1,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(lgbm_clf_final.named_steps[\"lgbm_classifier\"])\n",
    "print(\"\")\n",
    "print(rf_clf_final.named_steps[\"rf_classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
      "         transformer_weights=None,\n",
      "         transformers=[('tweet_transformer', Pipeline(memory=None,\n",
      "     steps=[('cleaner', tweet_cleaner(verbose=False)), ('tfidf_vectorizer', TfidfVectorizer(analyzer='w...0, subsample_freq=0))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft', weights=None))])\n"
     ]
    }
   ],
   "source": [
    "## Use a soft voting classifier to make final predictions from all models\n",
    "vote_clf = VotingClassifier(estimators=[\n",
    "                            (\"rf_clf\", rf_clf_final.named_steps[\"rf_classifier\"]),\n",
    "                            #(\"gb_clf\", gb_clf_final), \n",
    "                            #(\"xgb_clf\", xgb_clf_final),\n",
    "                            (\"lgbm_clf\", lgbm_clf_final.named_steps[\"lgbm_classifier\"])\n",
    "                            ], voting=\"soft\")\n",
    "\n",
    "vote_clf_fullpipe = Pipeline(steps=[\n",
    "                                (\"preprocessor\", preprocessor),\n",
    "                                (\"voting_classifier\", vote_clf)\n",
    "                            ])\n",
    "\n",
    "print(vote_clf_fullpipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VotingClassifier on 160000 samples...\n",
      "('voting_classifier', VotingClassifier(estimators=[('rf_clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=100,\n",
      "            ..., reg_lambda=0.0, silent=False,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0))],\n",
      "         flatten_transform=None, n_jobs=None, voting='soft', weights=None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 14.0min finished\n"
     ]
    }
   ],
   "source": [
    "vote_clf_final = pipeline_fit(pipeline=\"VotingClassifier\", cv=None, iid=False, verbose=2, n_jobs=1, optimization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 14.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Targets: [1 0 1 ... 1 1 1]\n",
      "Actual Targets: [0 0 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "#display(X_valid)\n",
    "y_pred = vote_clf_final.predict(X_valid)\n",
    "#y_pred = lgbm_clf_final.predict(X_valid)\n",
    "print(\"Predicted Targets:\",y_pred)\n",
    "print(\"Actual Targets:\",y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Targets: [0 1 1 ... 1 0 1]\n",
      "Actual Targets: [0 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred_2 = lgbm_clf_final.predict(X_valid_2)\n",
    "print(\"Predicted Targets:\",y_pred_2)\n",
    "print(\"Actual Targets:\",y_valid_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target names:  ['Negative' 'Positive']\n",
      "n_classes:  2\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.79      0.79    719965\n",
      "    Positive       0.79      0.79      0.79    720035\n",
      "\n",
      "   micro avg       0.79      0.79      0.79   1440000\n",
      "   macro avg       0.79      0.79      0.79   1440000\n",
      "weighted avg       0.79      0.79      0.79   1440000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[567825 152140]\n",
      " [153898 566137]]\n"
     ]
    }
   ],
   "source": [
    "#y_pred_inv = le.inverse_transform(y_pred)\n",
    "target_names = le.classes_\n",
    "print(\"Target names: \",target_names)\n",
    "n_classes = le.classes_.shape[0]\n",
    "print(\"n_classes: \",n_classes)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred, target_names=target_names))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_valid, y_pred, labels=range(n_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target names:  ['Negative' 'Positive']\n",
      "n_classes:  2\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.79      0.80    719801\n",
      "    Positive       0.79      0.80      0.80    720199\n",
      "\n",
      "   micro avg       0.80      0.80      0.80   1440000\n",
      "   macro avg       0.80      0.80      0.80   1440000\n",
      "weighted avg       0.80      0.80      0.80   1440000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[569211 150590]\n",
      " [142762 577437]]\n"
     ]
    }
   ],
   "source": [
    "target_names = le.classes_\n",
    "print(\"Target names: \",target_names)\n",
    "n_classes = le.classes_.shape[0]\n",
    "print(\"n_classes: \",n_classes)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid_2, y_pred_2, target_names=target_names))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_valid_2, y_pred_2, labels=range(n_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[568622 151343]\n",
      " [147019 573016]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3ea27d24e0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e89e77da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label'\n",
    "          )\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_valid, y_pred, classes=le.classes_, normalize=False,\n",
    "                      title=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
